{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b2862bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain core imports\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# LangChain specific imports\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Load environment variable\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae692721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from 15 ArXiv papers...\n",
      "\n",
      "âœ… Loaded 150 chunks from ArXiv papers\n",
      "\n",
      "Sample ArXiv paper:\n",
      "   Title: Agentic Test-Time Scaling for WebAgents\n",
      "   ArXiv ID: 2602.12276v1\n",
      "   Category: cs.AI\n",
      "   Section: Abstract\n",
      "âœ… Created 150 LangChain documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_arxiv_chunks(chunked_path):\n",
    "    \"\"\"Load ALL chunked ArXiv papers from data-ingestion pipeline\"\"\"\n",
    "    documents = []\n",
    "    json_files = sorted(chunked_path.glob(\"*.json\"))\n",
    "    \n",
    "    print(f\"Loading from {len(json_files)} ArXiv papers...\")\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r') as f:\n",
    "            chunks = json.load(f)\n",
    "            for chunk in chunks:\n",
    "                documents.append({\n",
    "                    'content': chunk['content'],\n",
    "                    'metadata': chunk['metadata']\n",
    "                })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Path to hybrid chunked data from data-ingestion pipeline\n",
    "chunked_data_path = Path(\"../../data-ingestion/processed/chunked/hybrid/\")\n",
    "arxiv_chunks = load_arxiv_chunks(chunked_data_path)\n",
    "\n",
    "print(f\"\\nLoaded {len(arxiv_chunks)} chunks from ArXiv papers\")\n",
    "print(f\"\\nSample ArXiv paper:\")\n",
    "print(f\"   Title: {arxiv_chunks[0]['metadata']['title']}\")\n",
    "print(f\"   ArXiv ID: {arxiv_chunks[0]['metadata']['arxiv_id']}\")\n",
    "print(f\"   Category: {arxiv_chunks[0]['metadata']['primary_category']}\")\n",
    "print(f\"   Section: {arxiv_chunks[0]['metadata']['section']}\")\n",
    "\n",
    "# Convert to LangChain Documents\n",
    "sample_documents = []\n",
    "for chunk in arxiv_chunks:\n",
    "    doc = Document(\n",
    "        page_content=chunk['content'],\n",
    "        metadata={\n",
    "            'arxiv_id': chunk['metadata']['arxiv_id'],\n",
    "            'title': chunk['metadata']['title'],\n",
    "            'category': chunk['metadata']['primary_category'],\n",
    "            'section': chunk['metadata']['section'],\n",
    "            'chunk_id': chunk['metadata']['chunk_id'],\n",
    "            'source': 'arxiv'\n",
    "        }\n",
    "    )\n",
    "    sample_documents.append(doc)\n",
    "\n",
    "print(f\"reated {len(sample_documents)} LangChain documents from all 100 papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354fa903",
   "metadata": {},
   "source": [
    "## Load Real ArXiv Research Papers\n",
    "\n",
    "Loading pre-chunked ArXiv papers from the data-ingestion pipeline to demonstrate FAISS vector store capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "264ed439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using 150 pre-chunked ArXiv documents\n",
      "\n",
      "Example chunk:\n",
      "Title: Agentic Test-Time Scaling for WebAgents...\n",
      "Content: Test-time scaling has become a standard way\n",
      "to improve performance and boost reliability of\n",
      "neural network models. However, its behavior\n",
      "on agentic, m...\n",
      "Metadata: {'arxiv_id': '2602.12276v1', 'title': 'Agentic Test-Time Scaling for WebAgents', 'category': 'cs.AI', 'section': 'Abstract', 'chunk_id': 0, 'source': 'arxiv'}\n"
     ]
    }
   ],
   "source": [
    "# ArXiv chunks are already optimally chunked from data-ingestion!\n",
    "# We'll use them directly without additional splitting\n",
    "\n",
    "chunks = sample_documents\n",
    "print(f\"âœ… Using {len(chunks)} pre-chunked ArXiv documents\")\n",
    "print(f\"\\nExample chunk:\")\n",
    "print(f\"Title: {chunks[0].metadata['title'][:60]}...\")\n",
    "print(f\"Content: {chunks[0].page_content[:150]}...\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "62addf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 150 chunks from 150 documents\n",
      "\n",
      "Example chunk:\n",
      "Content: Test-time scaling has become a standard way\n",
      "to improve performance and boost reliability of\n",
      "neural network models. However, its behavior\n",
      "on agentic, multi-step tasks remains less well-\n",
      "understood: small per-step errors can compound\n",
      "over long horizons; and we find that naive policies\n",
      "that uniformly increase sampling show diminish-\n",
      "ing returns. In this work, we present CATTS, a\n",
      "simple technique for dynamically allocating com-\n",
      "pute for multi-step agents. We first conduct an\n",
      "empirical study of inference-time scaling for web\n",
      "agents. We find that uniformly increasing per-\n",
      "step compute quickly saturates in long-horizon\n",
      "environments. We then investigate stronger ag-\n",
      "gregation strategies, including an LLM-based\n",
      "Arbiter that can outperform naive voting, but\n",
      "that can overrule high-consensus decisions. We\n",
      "show that uncertainty statistics derived from the\n",
      "agentâ€™s own vote distribution (entropy and top-\n",
      "1/top-2 margin) correlate with downstream suc-\n",
      "cess and provide a practical signal for dynamic\n",
      "compute allocation. Based on these findings, we\n",
      "introduce Confidence-Aware Test-Time Scaling\n",
      "(CATTS), which uses vote-derived uncertainty to\n",
      "allocate compute only when decisions are gen-\n",
      "uinely contentious.\n",
      "CATTS improves perfor-\n",
      "mance on WebArena-Lite and GoBrowse by up\n",
      "to 9.1% over React while using up to 2.3Ã— fewer\n",
      "tokens than uniform scaling, providing both effi-\n",
      "ciency gains and an interpretable decision rule.\n",
      "Metadata: {'arxiv_id': '2602.12276v1', 'title': 'Agentic Test-Time Scaling for WebAgents', 'category': 'cs.AI', 'section': 'Abstract', 'chunk_id': 0, 'source': 'arxiv'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Created {len(chunks)} chunks from {len(sample_documents)} documents\")\n",
    "print(\"\\nExample chunk:\")\n",
    "print(f\"Content: {chunks[0].page_content}\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5fcb8176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace Embeddings Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace embeddings - Free, runs locally!\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "print(\"HuggingFace Embeddings Loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "af35465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[\"AI\",\"MAchine learning\",\"Deep Learning\",\"Neural Network\"]\n",
    "batch_embeddings=embeddings.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "56377286",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compare Embedding using cosine similarity\n",
    "\n",
    "def compare_embeddings(text1:str,text2:str):\n",
    "    \"\"\"Compare semantic simialrity of 2 texts usign embeddings\"\"\"\n",
    "\n",
    "    emb1=np.array(embeddings.embed_query(text1))\n",
    "    emb2=np.array(embeddings.embed_query(text2))\n",
    "\n",
    "    ## Calculate the simialrity score\n",
    "\n",
    "    similarity=np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "39877a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic Similarity Examples:\n",
      "'AI' vs 'Artificial Intelligence': 0.791\n"
     ]
    }
   ],
   "source": [
    "# Test semantic similarity\n",
    "print(\"\\nSemantic Similarity Examples:\")\n",
    "print(f\"'AI' vs 'Artificial Intelligence': {compare_embeddings('AI', 'Artificial Intelligence'):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9ffc4095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with 150 vectors\n"
     ]
    }
   ],
   "source": [
    "### Create FAISS Vector Store\n",
    "vectorstore=FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "print(f\"Vector store created with {vectorstore.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "66145250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store saved to 'faiss_index' directory\n"
     ]
    }
   ],
   "source": [
    "## Save vector tore for later use\n",
    "vectorstore.save_local(\"data/faiss_index\")\n",
    "print(\"Vector store saved to 'faiss_index' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "897ee9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vector store contains 150 vectors\n"
     ]
    }
   ],
   "source": [
    "## load vector store\n",
    "loaded_vectorstore=FAISS.load_local(\n",
    "    \"data/faiss_index\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(f\"Loaded vector store contains {loaded_vectorstore.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8723a0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Query: What are neural networks and deep learning architectures?\n",
      "\n",
      "Top 3 ArXiv papers:\n",
      "\n",
      "1. A technical curriculum on language-oriented artificial intelligen...\n",
      "   ArXiv: 2602.12251v1 | Section: approach\n",
      "   Content: The technical curriculum on language-oriented AI\n",
      "in translation and specialised communication is\n",
      "provided as a GitHub repository2, which consists\n",
      "of a...\n",
      "\n",
      "2. Energy-Aware Spike Budgeting for Continual Learning in Spiking Ne...\n",
      "   ArXiv: 2602.12236v1 | Section: 1\n",
      "Introduction\n",
      "   Content: Neuromorphic computing has emerged as a paradigm shift in artificial intelligence, drawing\n",
      "inspiration from the energy efficiency and temporal dynamic...\n",
      "\n",
      "3. Energy-Aware Spike Budgeting for Continual Learning in Spiking Ne...\n",
      "   ArXiv: 2602.12236v1 | Section: methods\n",
      "   Content: IOP Publishing\n",
      "Journal vv (yyyy) aaaaaa\n",
      "Author et al\n",
      "uncertainty-aware learning, context-aware mechanisms [16] that adaptively modulate network\n",
      "resour...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Similarity Search on ArXiv Papers\n",
    "query = \"What are neural networks and deep learning architectures?\"\n",
    "\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"ðŸ” Query: {query}\\n\")\n",
    "print(\"Top 3 ArXiv papers:\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.metadata['title'][:65]}...\")\n",
    "    print(f\"   ArXiv: {doc.metadata['arxiv_id']} | Section: {doc.metadata['section']}\")\n",
    "    print(f\"   Content: {doc.page_content[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f017539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Query: machine learning algorithms and optimization techniques\n",
      "\n",
      "Results with similarity scores:\n",
      "\n",
      "1. Similarity: 1.2782\n",
      "   Paper: Mixed-Integer Programming for Change-point Detection...\n",
      "   ArXiv: 2602.11947v1 | Category: math.OC\n",
      "   Content: signal of the data, and the term breakpoint to denote the corresponding decision variable in a PWL\n",
      "model at which model ...\n",
      "\n",
      "2. Similarity: 1.3002\n",
      "   Paper: dVoting: Fast Voting for dLLMs...\n",
      "   ArXiv: 2602.12153v1 | Category: cs.CL\n",
      "   Content: demonstrate\n",
      "that our method boosts the performance across all bench-\n",
      "marks covering mathematical, scientific, and genera...\n",
      "\n",
      "3. Similarity: 1.3437\n",
      "   Paper: On the implicit regularization of Langevin dynamics with pro...\n",
      "   ArXiv: 2602.12257v1 | Category: math.PR\n",
      "   Content: A central feature of modern machine learning is that models are often heavily over-\n",
      "parameterized yet still achieve exce...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Similarity Search with Scores - Another Query\n",
    "query2 = \"machine learning algorithms and optimization techniques\"\n",
    "results_with_scores = vectorstore.similarity_search_with_score(query2, k=3)\n",
    "\n",
    "print(f\"ðŸ” Query: {query2}\\n\")\n",
    "print(\"Results with similarity scores:\\n\")\n",
    "for i, (doc, score) in enumerate(results_with_scores, 1):\n",
    "    print(f\"{i}. Similarity: {score:.4f}\")\n",
    "    print(f\"   Paper: {doc.metadata['title'][:60]}...\")\n",
    "    print(f\"   ArXiv: {doc.metadata['arxiv_id']} | Category: {doc.metadata['category']}\")\n",
    "    print(f\"   Content: {doc.page_content[:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "015089f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Query: computational methods and algorithms\n",
      "\n",
      "1. dVoting: Fast Voting for dLLMs...\n",
      "   Category: cs.CL | Section: results\n",
      "   Preview: demonstrate\n",
      "that our method boosts the performance across all bench-\n",
      "marks covering mathematical, sc...\n",
      "\n",
      "2. Mixed-Integer Programming for Change-point Detection...\n",
      "   Category: math.OC | Section: 1\n",
      "Introduction\n",
      "   Preview: segment each data point belongs to. Therefore, by explicitly encoding segment assignments, MIP\n",
      "formu...\n",
      "\n",
      "3. Mixed-Integer Programming for Change-point Detection...\n",
      "   Category: math.OC | Section: experiments\n",
      "   Preview: perspective, the key combinatorial decision concerns the optimal partitioning of data points into\n",
      "co...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Search with Metadata Filtering - Filter by ArXiv category\n",
    "query3 = \"computational methods and algorithms\"\n",
    "\n",
    "# Try to filter by category (if papers have similar categories)\n",
    "filtered_results = vectorstore.similarity_search(\n",
    "    query3,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(f\"ðŸ” Query: {query3}\\n\")\n",
    "for i, doc in enumerate(filtered_results, 1):\n",
    "    print(f\"{i}. {doc.metadata['title'][:65]}...\")\n",
    "    print(f\"   Category: {doc.metadata['category']} | Section: {doc.metadata['section']}\")\n",
    "    print(f\"   Preview: {doc.page_content[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1adf8ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… HuggingFace LLM Loaded!\n",
      "   Model: HuggingFaceH4/zephyr-7b-beta\n"
     ]
    }
   ],
   "source": [
    "## LLM - HuggingFace Model (Chat-based)\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "# Set HuggingFace API token from .env\n",
    "huggingface_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = huggingface_token\n",
    "\n",
    "# Initialize HuggingFace LLM via chat endpoint\n",
    "endpoint = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    huggingfacehub_api_token=huggingface_token,\n",
    ")\n",
    "\n",
    "llm = ChatHuggingFace(llm=endpoint)\n",
    "\n",
    "print(\"âœ… HuggingFace LLM Loaded!\")\n",
    "print(f\"   Model: HuggingFaceH4/zephyr-7b-beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3b61ae09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Test Response:\n",
      "AI, or artificial intelligence, refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, reasoning, and decision-making. It involves the use of sophisticated algorithms and machine learning techniques to enable computers to process and analyze large amounts of data, identify patterns, and make informed decisions or suggestions based on that data. AI systems can adapt and learn over time, improving their performance as they are exposed to more information, without being explicitly programmed. AI has the potential to revolutionize various industries and improve efficiency, accuracy, and productivity in various applications, such as healthcare, finance, transportation, and manufacturing, among others. It encompasses various subfields like natural language processing, computer vision, robotics, and machine learning, which enable computers to understand and interpret human language, recognize objects and images, and control physical equipment, respectively. AI systems can also work cooperatively with humans, known as collaborative AI or augmented intelligence, or autonomously, known as autonomous AI. Ultimately, AI aims to create intelligent machines that can operate and make decisions without the need for human intervention, known as general AI or artificial general intelligence. However, the technology is still in its nascent stages, and there are challenges with its implementation, including data privacy, trust, and safety concerns, as well as job displacement due to automation.\n"
     ]
    }
   ],
   "source": [
    "# Test the LLM\n",
    "response = llm.invoke(\"What is AI?\")\n",
    "print(\"LLM Test Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2b1e5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Simple RAG Chain with LCEL\n",
    "simple_prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on the following context:\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "417531aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic retriever\n",
    "retriever=vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "89d52bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "# Format documents for the prompt\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"Format documents for insertion into prompt\"\"\"\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        formatted.append(f\"Document {i+1} (Source: {source}):\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "060dc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | simple_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0e5d7b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conversational RAg Chain\n",
    "\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use the provided context to answer questions.\"),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nQuestion: {input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a4140609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversational_rag():\n",
    "    \"\"\"Create a conversational RAG chain with memory\"\"\"\n",
    "    return (\n",
    "        RunnablePassthrough.assign(\n",
    "            context=lambda x: format_docs(retriever.invoke(x[\"input\"]))\n",
    "        )\n",
    "        | conversational_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "conversational_rag = create_conversational_rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "60c212cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chains created successfully!\n",
      "Available chains:\n",
      "- simple_rag_chain: Basic Q&A\n",
      "- conversational_rag: Maintains conversation history\n",
      "- streaming_rag_chain: Supports token streaming\n"
     ]
    }
   ],
   "source": [
    "### streaming RAG chain\n",
    "streaming_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | simple_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chains created successfully!\")\n",
    "print(\"Available chains:\")\n",
    "print(\"- simple_rag_chain: Basic Q&A\")\n",
    "print(\"- conversational_rag: Maintains conversation history\")\n",
    "print(\"- streaming_rag_chain: Supports token streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8b40c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for different chain types\n",
    "def test_rag_chains(question: str):\n",
    "    \"\"\"Test all RAG chain variants\"\"\"\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Simple RAG\n",
    "    print(\"\\n1. Simple RAG Chain:\")\n",
    "    answer = simple_rag_chain.invoke(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "    # 2. Streaming RAG\n",
    "    print(\"\\n2. Streaming RAG:\")\n",
    "    print(\"Answer: \", end=\"\", flush=True)\n",
    "    for chunk in streaming_rag_chain.stream(question):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0240f0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the difference between AI and machine learning\n",
      "================================================================================\n",
      "\n",
      "1. Simple RAG Chain:\n",
      "Answer: Based on the provided context, it is clear that there is a distinction between AI and machine learning. While AI technologies such as LLMs (Language Language Models) and NMT (Neural Machine Translation) are being widely adopted in various industries due to their practical applications, there is a call for promoting technical AI literacy among industry stakeholders to better understand their underlying principles rather than just relying on their performance. This is in contrast to relying solely on their results. The importance of technical AI literacy is emphasized by authors such as Doherty and Kenny in the context of SMT (Statistical Machine Translation) and neural machine translation (NMT) to dispel the mystery surrounding these technologies. Additionally, Shneiderman suggests that industry stakeholders require a deeper understanding of AI in general, specifically in the context of language-oriented AI and its subdimensions of technical, performance-focused, interaction-focused, and ethical/societal AI literacy, highlighting that AI is already capable of solving language problems to a satisfactory extent and will continue to improve in performance. However, a separate curriculum addressing the technical foundations of language-oriented AI, including vector embeddings and neural networks, is being provided through Jupyter notebooks hosted on Google's Colab environment, presented in the form of a GitHub repository under a CC BY-SA 4.0 License. This curriculum is intended to enhance literacy in language-oriented AI for the L&T (Language and Translation) industry. Therefore, the difference between AI and machine learning is that while machine learning refers specifically to the algorithms and models used to train and improve AI systems, AI encompasses a broader set of principles and concepts related to their implementation, performance, and societal implications.\n",
      "\n",
      "2. Streaming RAG:\n",
      "Answer: Based on the provided context, the difference between AI and machine learning is not directly addressed or implied in the given material. The text discusses technical literacy in AI, which appears to encompass machine learning as a specific aspect of AI, but does not provide a clear distinction between the two concepts. However, it can be inferred that AI encompasses a broader understanding of intelligent systems beyond just machine learning algorithms, while machine learning refers to specific techniques and technologies used in AI systems, as mentioned in the context of LLMs (language language models) and neural machine translation (NMT). The authors Doherty and Kenny argue that technical literacy helps demystify the \"black-box\" nature of these technologies, implying that AI systems may not be fully understood just by their use alone. Additionally, in the context of language translation, Shneiderman advocates for domain-specific AI literacy, which includes technical, performance-focused, interaction-focused, and ethical/societal dimensions. The material also presents a technical curriculum focused on language-oriented AI, highlighting vector embeddings and neural network foundations, but does not explicitly distinguish between AI and machine learning in this context. Therefore, it can be inferred that AI goes beyond the scope of machine learning in language processing applications. However, it is not explicitly stated whether AI and machine learning are synonymous or distinct concepts.\n"
     ]
    }
   ],
   "source": [
    "test_rag_chains(\"What is the difference between AI and machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "06037111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Question: What is the difference between AI and Machine Learning?\n",
      "================================================================================\n",
      "\n",
      "1. Simple RAG Chain:\n",
      "Answer: Based on the given context, there seems to be no direct question asking about the difference between AI and machine learning. However, the term \"technical AI literacy\" is used interchangeably with \"machine learning\" in Document 1, suggesting that AI and machine learning may be used synonymously in that context. Therefore, it can be inferred that AI and machine learning are referring to the same concept in this case. Therefore, there is no clear distinction between the two in this specific context. However, in Document 2, the term \"AI literacy\" is further broken down into subcategories such as technical, performance-focused, interaction-focused, and ethical/societal literacy, implying that AI encompasses more than just the technical aspects of machine learning. Therefore, it can be inferred that AI may go beyond just machine learning, as those subcategories suggest broader concepts related to AI beyond just the technical foundations. In Document 3, a curriculum on \"language-oriented AI\" is presented, which includes both technical foundations of neural networks and vector embeddings, indicating that AI and machine learning in the context of language processing and natural language processing may involve more than just neural networks and vector embeddings. Overall, while the terms AI and machine learning may be used interchangeably in some contexts, AI seems to encompass a broader concept that includes more than just technical aspects, as seen in Document 2's breakdown of AI literacy into subcategories. Therefore, it can be inferred that AI goes beyond just machine learning, but the curriculum in Document 3 focuses specifically on language-oriented AI, which includes both machine learning and its technical foundations. However, further research or context is needed to confirm if the use of AI and machine learning are synonymous in this specific context.\n",
      "\n",
      "2. Streaming RAG:\n",
      "Answer: Based on the context provided, the given question cannot be answered as the texts provided do not directly compare or contrast AI and Machine Learning. The first document discusses the importance of technical AI literacy for industry stakeholders to better understand the foundations of AI systems, specifically focusing on language technologies such as language models like LLMs and NMT, rather than solely relying on their use. The second document emphasizes the need for domain-specific AI literacy, which includes subdimensions of technical, performance-focused, interaction-focused, and ethical/societal literacy in the L&T industry, highlighting the current capabilities and potential improvement of these systems. The third document introduces a technical curriculum on language-oriented AI, including vector embeddings and neural networks, without directly comparing or contrasting AI and Machine Learning. Therefore, there is no clear distinction drawn between the two terms in this context. However, it should be noted that Machine Learning is a subfield of AI that focuses on training algorithms to learn and improve performance on specific tasks through data, while AI encompasses a broader range of technologies, including but not limited to Machine Learning algorithms.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question: Explain deep learning in simple terms\n",
      "================================================================================\n",
      "\n",
      "1. Simple RAG Chain:\n",
      "Answer: The first document is about representing linguistic data as numbers using vector embeddings in neural networks, specifically discussing both static and dynamic word embeddings as well as sentence embeddings for multiple languages. The second document introduces a group symmetry assumption to explain over-parameterization in deep learning models, where the loss function is invariant under certain transformations. The third document highlights the importance of technical literacy in understanding deep learning, particularly in the context of Large Language Models (LLMs) and neural machine translation (NMT). While the first document focuses on vector embeddings, the second discusses over-parameterization, but neither directly addresses deep learning. However, the third document mentions LLMs and NMT as examples of deep learning technologies that can benefit from technical literacy. Therefore, to answer the question, we can say that deep learning involves representing linguistic data as numerical vectors using techniques like static and dynamic word embeddings, and LLMs and NMT can benefit from technical literacy due to their symmetry properties that result in orthogonal gradients during training. This highlights the importance of understanding the technical foundations of deep learning and its underlying principles beyond just using these technologies. However, the second document does not explicitly discuss deep learning in simple terms.\n",
      "\n",
      "2. Streaming RAG:\n",
      "Answer: \n",
      "The first document focuses on representing linguistic data as numbers using vector embeddings, which allows neural networks to process it. It covers static and dynamic word embeddings, as well as sentence embeddings for multiple languages. Users can load pre-trained static word embeddings and also learn their own models. The second document discusses the concept of over-parameterization in deep learning, where the loss function is invariant under a group symmetry, and the gradients for training are orthogonal to the orbit of the input. The third document mentions the use of transformer neural networks, which are commonly used in AI, but also encourages technical literacy in the field to better understand these technologies beyond just their applications. This is important for stakeholders in industry as it sheds light on their inner workings and helps demystify the black-box nature of popular models like LLMs and NMTs. The context provides a brief overview of vector embeddings, neural networks, and tokenization, which involves splitting words into smaller parts for processing. Overall, these documents cover various aspects of deep learning techniques.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question: How does NLP work?\n",
      "================================================================================\n",
      "\n",
      "1. Simple RAG Chain:\n",
      "Answer: Based on the given context, NLP (Natural Language Processing) is limited in both training and evaluation due to the scarcity of domain-specific resources adapted to municipal discourse, as many researchers resort to using web-scraped data, which can be noisy and may contain sensitive information or copyright concerns, hindering progress on specific tasks. This limitation is further exacerbated by the lack of multilayer annotations that are essential for deeper linguistic analysis and effective information retrieval research. However, the introduction of the CitiLink-Minutes dataset addresses this issue by providing annotated municipal meeting minutes that are more challenging for both humans and computational systems due to their lengthy and heterogeneous structures, making it difficult to locate and compare relevant information, such as policy topics and voting outcomes. This scarcity hinders progress in tasks like metadata extraction, topic classification, and vote labeling. CitiLink-Minutes demonstrates the potential for downstream NLP and IR tasks and promotes transparent access to municipal decisions.\n",
      "\n",
      "2. Streaming RAG:\n",
      "Answer: The context provided does not directly answer the question about how NLP (Natural Language Processing) works. It only highlights the limitation in NLP research due to the scarcity of municipal discourse data, as many researchers rely on web-scraped data that may be noisy and contain sensitive information or copyright concerns, which hinders model training and limits progress on domain-specific tasks that require resources tailored to the linguistic and structural features of municipal discourse. It also mentions the scarcity of multilayer annotations, which is essential for deeper linguistic analysis and effective IR research. The paper introduces the CitiLink-Minutes dataset, which addresses this gap by providing annotated municipal meeting minutes that have structures that vary across municipalities and are challenging for both humans and computational systems to locate, extract, and compare information due to their lengthiness and heterogeneity. The paper suggests that progress in IR and NLP tasks, such as metadata extraction and topic classification, can be promoted through the availability of municipal meeting minutes, as they are rarely curated, standardized, and openly available for computational research. This dataset demonstrates its potential for downstream NLP and IR tasks, making transparent access to municipal decisions possible. Therefore, NLP involves the extraction of information and classification from municipal meeting minutes, which are often lengthy and heterogeneous, with structures that vary in municipalities, making it difficult for both humans and computational systems to locate and compare information due to their lengthiness and variability. The context doesn't provide a detailed explanation of how NLP works, but rather highlights the limitations and the need for high-quality, multilayer annotated data for both humans and computational systems in dealing with municipal meeting minutes, which are rarely curated and openly available for computational research.\n"
     ]
    }
   ],
   "source": [
    "# Test with multiple questions\n",
    "test_questions = [\n",
    "    \"What is the difference between AI and Machine Learning?\",\n",
    "    \"Explain deep learning in simple terms\",\n",
    "    \"How does NLP work?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    test_rag_chains(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "85ab0dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Conversational RAG Example:\n",
      "Q1: What is machine learning?\n",
      "A1: \n",
      "Based on the context provided, machine learning is a type of artificial intelligence technique that involves heavily parameterized models with over-fitting capabilities, which can achieve excellent generalization performance even when there are fewer parameters than training data points. This is in contrast to classical statistical theory that suggests the need for regularization to prevent overfitting in heavily parameterized models. This concept has been observed in practice, as explained in Document 2. Additionally, in Document 1, the use of machine learning systems is praised for demystifying their black-box nature, particularly in the contexts of language translation and neural machine translation. There is a call for fostering technical AI literacy among industry stakeholders to better understand these technologies beyond just their applications, as argued by Doherty and Kenny in statistical machine translation and Kenny in neural machine translation. Document 3 touches on a theory called complementary learning systems, which suggests that the neocortex learns slow statistical regularities and the hippocampus rapidly encodes specific conjunctions, while the retrieval of certain experiences may be triggered by distinct features such as the smell of sunscreen for a beach or the quality of afternoon light for a specific memory, which is complementary to the hippocampus's rapid encoding of those experiences. This suggests that machine learning involves the ability to learn and recall complex associations between experiences based on their unique features. Overall, the context provided highlights the benefits of technical AI literacy for industry stakeholders to better understand the workings of these technologies beyond just their applications.\n"
     ]
    }
   ],
   "source": [
    "## Conversational example\n",
    "print(\"\\n3. Conversational RAG Example:\")\n",
    "chat_history = []\n",
    "\n",
    "# First question\n",
    "q1 = \"What is machine learning?\"\n",
    "a1 = conversational_rag.invoke({\n",
    "    \"input\": q1,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "print(f\"Q1: {q1}\")\n",
    "print(f\"A1: {a1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f85f3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update history\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=q1),\n",
    "    AIMessage(content=a1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "182412dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q2: How is it different from traditional programming?\n",
      "A2: \n",
      "Based on the provided context, machine learning differs from traditional programming in that it involves heavily parameterized models with the ability to achieve excellent generalization performance even when there are fewer training data points through over-fitting, unlike traditional programming techniques that may require regularization to prevent overfitting due to the need for fewer parameters. This concept is explained in Document 2. Additionally, machine learning systems are being increasingly adopted in language translation and neural machine translation (NMT), as discussed in Documents 1 and 3. While traditional programming focuses on writing explicit instructions for computers to perform specific tasks, machine learning allows computers to learn and make predictions or decisions based on patterns and inputs without explicit instructions, relying on large amounts of data to improve their performance over time through learning. This is different from traditional programming, where algorithms are explicitly programmed to perform specific tasks. The provided context highlights the need for industry stakeholders to better understand machine learning beyond just its applications through technical literacy, as argued in Documents 1 and 3, as they involve vector embeddings, the technical foundations of neural networks, and tokenization, ultimately contributing to digital resilience in highly automated environments. The curriculum presented in Document 2 is provided as a GitHub repository and aims to foster technical AI literacy among stakeholders in the language and translation industry, with additional aspects of language-oriented AI being added in the future. While traditional programming involves writing instructions for computers to perform specific tasks, machine learning involves learning complex associations between experiences based on their unique features, as suggested by the theory of complementary learning systems (Document 3). The curriculum presented in Document 2 is being tested in an AI-focused course, and participant feedback suggests that it should be embedded in higher-level didactic scaffolding, such as lecturer support, for optimal learning conditions. This curriculum focuses on the technical foundations of language-oriented AI in translation and specialised communication, presented in a series of Jupyter notebooks and covers vector embeddings, the technical foundations of neural networks, and tokenization, ultimately contributing to digital resilience in highly automated environments, as supported by research in language translation and neural machine translation. The curriculum presented in Document 2 is available under a CC BY-SA 4.0 License and hosted on Google's Colab online environment. The curriculum presented in Document 3 highlights the need for technical AI literacy in the language and translation industry, as supported by research in agency and adapt\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question\n",
    "q2 = \"How is it different from traditional programming?\"\n",
    "a2 = conversational_rag.invoke({\n",
    "    \"input\": q2,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "print(f\"\\nQ2: {q2}\")\n",
    "print(f\"A2: {a2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
