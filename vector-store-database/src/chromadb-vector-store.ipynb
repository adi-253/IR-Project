{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d3eeaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain core imports\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# LangChain specific imports\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Load environment variable\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3de00e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from 15 ArXiv papers (max 10 chunks each)...\n",
      "\n",
      "âœ… Loaded 150 chunks from ArXiv papers\n",
      "\n",
      "Sample ArXiv paper:\n",
      "   Title: Amortized Inference of Neuron Parameters on Analog Neuromorphic Hardware\n",
      "   ArXiv ID: 2602.10763v2\n",
      "   Category: cs.NE\n",
      "   Section: Abstract\n",
      "âœ… Created 150 LangChain documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_arxiv_chunks(chunked_path, max_files=15, chunks_per_file=10):\n",
    "    \"\"\"Load chunked ArXiv papers from data-ingestion pipeline (limited for testing)\"\"\"\n",
    "    documents = []\n",
    "    json_files = sorted(chunked_path.glob(\"*.json\"))[:max_files]\n",
    "    \n",
    "    print(f\"Loading from {len(json_files)} ArXiv papers (max {chunks_per_file} chunks each)...\")\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r') as f:\n",
    "            chunks = json.load(f)\n",
    "            for chunk in chunks[:chunks_per_file]:\n",
    "                documents.append({\n",
    "                    'content': chunk['content'],\n",
    "                    'metadata': chunk['metadata']\n",
    "                })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Path to hybrid chunked data from data-ingestion pipeline\n",
    "chunked_data_path = Path(\"../../data-ingestion/processed/chunked/hybrid/\")\n",
    "arxiv_chunks = load_arxiv_chunks(chunked_data_path, max_files=15, chunks_per_file=10)\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(arxiv_chunks)} chunks from ArXiv papers\")\n",
    "print(f\"\\nSample ArXiv paper:\")\n",
    "print(f\"   Title: {arxiv_chunks[0]['metadata']['title']}\")\n",
    "print(f\"   ArXiv ID: {arxiv_chunks[0]['metadata']['arxiv_id']}\")\n",
    "print(f\"   Category: {arxiv_chunks[0]['metadata']['primary_category']}\")\n",
    "print(f\"   Section: {arxiv_chunks[0]['metadata']['section']}\")\n",
    "\n",
    "# Convert to LangChain Documents\n",
    "sample_documents = []\n",
    "for chunk in arxiv_chunks:\n",
    "    doc = Document(\n",
    "        page_content=chunk['content'],\n",
    "        metadata={\n",
    "            'arxiv_id': chunk['metadata']['arxiv_id'],\n",
    "            'title': chunk['metadata']['title'],\n",
    "            'category': chunk['metadata']['primary_category'],\n",
    "            'section': chunk['metadata']['section'],\n",
    "            'chunk_id': chunk['metadata']['chunk_id'],\n",
    "            'source': 'arxiv'\n",
    "        }\n",
    "    )\n",
    "    sample_documents.append(doc)\n",
    "\n",
    "print(f\"âœ… Created {len(sample_documents)} LangChain documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd98c434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using 150 pre-chunked ArXiv documents\n",
      "\n",
      "Example chunk:\n",
      "Title: Amortized Inference of Neuron Parameters on Analog Neuromorp...\n",
      "Content: â€”Our work utilized a non-sequential simulation-based\n",
      "inference algorithm to provide an amortized neural density\n",
      "estimator, which approximates the post...\n",
      "Metadata: {'arxiv_id': '2602.10763v2', 'title': 'Amortized Inference of Neuron Parameters on Analog Neuromorphic Hardware', 'category': 'cs.NE', 'section': 'Abstract', 'chunk_id': 0, 'source': 'arxiv'}\n"
     ]
    }
   ],
   "source": [
    "# ArXiv chunks are already optimally chunked from data-ingestion\n",
    "chunks = sample_documents\n",
    "print(f\"âœ… Using {len(chunks)} pre-chunked ArXiv documents\")\n",
    "print(f\"\\nExample chunk:\")\n",
    "print(f\"Title: {chunks[0].metadata['title'][:60]}...\")\n",
    "print(f\"Content: {chunks[0].page_content[:150]}...\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26105c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… HuggingFace Embeddings Loaded!\n",
      "   Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "   Embedding Dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace embeddings - Free, runs locally!\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84916560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Similarity Examples:\n",
      "'AI' vs 'Artificial Intelligence': 0.791\n",
      "'AI' vs 'Cooking recipes': 0.152\n"
     ]
    }
   ],
   "source": [
    "### Compare Embedding using cosine similarity\n",
    "\n",
    "def compare_embeddings(text1: str, text2: str):\n",
    "    \"\"\"Compare semantic similarity of 2 texts using embeddings\"\"\"\n",
    "    emb1 = np.array(embeddings.embed_query(text1))\n",
    "    emb2 = np.array(embeddings.embed_query(text2))\n",
    "    similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "    return similarity\n",
    "\n",
    "# Test semantic similarity\n",
    "print(\"Semantic Similarity Examples:\")\n",
    "print(f\"'AI' vs 'Artificial Intelligence': {compare_embeddings('AI', 'Artificial Intelligence'):.3f}\")\n",
    "print(f\"'AI' vs 'Cooking recipes': {compare_embeddings('AI', 'Cooking recipes'):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "623fef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ChromaDB vector store created with 150 vectors\n",
      "   Persisted to: ./data/chroma_db\n"
     ]
    }
   ],
   "source": [
    "### Create ChromaDB Vector Store\n",
    "persist_directory = \"./data/chroma_db\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=\"arxiv_papers\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… ChromaDB vector store created with {vectorstore._collection.count()} vectors\")\n",
    "print(f\"   Persisted to: {persist_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "462db7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded vector store contains 150 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68462/3628335856.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  loaded_vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "## Load vector store from disk\n",
    "loaded_vectorstore = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"arxiv_papers\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Loaded vector store contains {loaded_vectorstore._collection.count()} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da1e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Query: What are neural networks and deep learning architectures?\n",
      "\n",
      "Top 3 ArXiv papers:\n",
      "\n",
      "1. CL API: Real-Time Closed-Loop Interactions with Biological Neural...\n",
      "   ArXiv: 2602.11632v1 | Section: methods\n",
      "   Content: network (BNN) that can be harnessed directly in controllable ways as exemplified in the fields of\n",
      "Synthetic Biological Intelligence (SBI) [1â€“3] and Or...\n",
      "\n",
      "2. Calibrated Bayesian Deep Learning for Explainable Decision Suppor...\n",
      "   ArXiv: 2602.11973v1 | Section: 1\n",
      "Introduction\n",
      "   Content: Deep learning has achieved remarkable advances in medical image analysis, demonstrating classification performance\n",
      "comparable to that of human experts...\n",
      "\n",
      "3. CL API: Real-Time Closed-Loop Interactions with Biological Neural...\n",
      "   ArXiv: 2602.11632v1 | Section: Abstract\n",
      "   Content: Biological neural networks (BNNs) are increasingly explored for their rich dy-\n",
      "namics, parallelism, and adaptive behavior. Beyond understanding their ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Similarity Search on ArXiv Papers\n",
    "query = \"What are neural networks and deep learning architectures?\"\n",
    "\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 3 ArXiv papers:\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.metadata['title'][:65]}...\")\n",
    "    print(f\"   ArXiv: {doc.metadata['arxiv_id']} | Section: {doc.metadata['section']}\")\n",
    "    print(f\"   Content: {doc.page_content[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90b067c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: machine learning algorithms and optimization techniques\n",
      "\n",
      "Results with similarity scores:\n",
      "\n",
      "1. Similarity: 1.2782\n",
      "   Paper: Mixed-Integer Programming for Change-point Detection...\n",
      "   ArXiv: 2602.11947v1 | Category: math.OC\n",
      "   Content: signal of the data, and the term breakpoint to denote the corresponding decision variable in a PWL\n",
      "model at which model ...\n",
      "\n",
      "2. Similarity: 1.2935\n",
      "   Paper: Evolution With Purpose: Hierarchy-Informed Optimization of W...\n",
      "   ArXiv: 2602.11398v1 | Category: cs.NE\n",
      "   Content: . While\n",
      "all heterogeneous strategies fit the data well, only curricular ap-\n",
      "proaches generalized to new subjects. Most i...\n",
      "\n",
      "3. Similarity: 1.3441\n",
      "   Paper: Amortized Inference of Neuron Parameters on Analog Neuromorp...\n",
      "   ArXiv: 2602.10763v2 | Category: cs.NE\n",
      "   Content: include\n",
      "directly deriving model parameters by fitting experimental\n",
      "observations, genetic algorithms, gradient-based...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Similarity Search with Scores\n",
    "query2 = \"machine learning algorithms and optimization techniques\"\n",
    "results_with_scores = vectorstore.similarity_search_with_score(query2, k=3)\n",
    "\n",
    "print(f\"Query: {query2}\\n\")\n",
    "print(\"Results with similarity scores:\\n\")\n",
    "for i, (doc, score) in enumerate(results_with_scores, 1):\n",
    "    print(f\"{i}. Similarity: {score:.4f}\")\n",
    "    print(f\"   Paper: {doc.metadata['title'][:60]}...\")\n",
    "    print(f\"   ArXiv: {doc.metadata['arxiv_id']} | Category: {doc.metadata['category']}\")\n",
    "    print(f\"   Content: {doc.page_content[:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af7f1c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Query: computational methods and algorithms (filtered: source=arxiv)\n",
      "\n",
      "1. Amortized Inference of Neuron Parameters on Analog Neuromorphic H...\n",
      "   Category: cs.NE | Section: INTRODUCTION\n",
      "   Preview: Identifying suitable models to replicate physical observations\n",
      "is often a laborious process. Even wh...\n",
      "\n",
      "2. Amortized Inference of Neuron Parameters on Analog Neuromorphic H...\n",
      "   Category: cs.NE | Section: methods\n",
      "   Preview: include\n",
      "directly deriving model parameters by fitting experimental\n",
      "observations, genetic algorithms,...\n",
      "\n",
      "3. Mixed-Integer Programming for Change-point Detection...\n",
      "   Category: math.OC | Section: 1\n",
      "Introduction\n",
      "   Preview: segment each data point belongs to. Therefore, by explicitly encoding segment assignments, MIP\n",
      "formu...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Search with Metadata Filtering - ChromaDB supports native filtering!\n",
    "query3 = \"computational methods and algorithms\"\n",
    "\n",
    "# ChromaDB supports where filters on metadata\n",
    "filtered_results = vectorstore.similarity_search(\n",
    "    query3,\n",
    "    k=3,\n",
    "    filter={\"source\": \"arxiv\"}\n",
    ")\n",
    "\n",
    "print(f\"ðŸ” Query: {query3} (filtered: source=arxiv)\\n\")\n",
    "for i, doc in enumerate(filtered_results, 1):\n",
    "    print(f\"{i}. {doc.metadata['title'][:65]}...\")\n",
    "    print(f\"   Category: {doc.metadata['category']} | Section: {doc.metadata['section']}\")\n",
    "    print(f\"   Preview: {doc.page_content[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c923410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… HuggingFace LLM Loaded!\n",
      "   Model: HuggingFaceH4/zephyr-7b-beta\n"
     ]
    }
   ],
   "source": [
    "## LLM - HuggingFace Model (Chat-based)\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "# Set HuggingFace API token from .env\n",
    "huggingface_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = huggingface_token\n",
    "\n",
    "# Initialize HuggingFace LLM via chat endpoint\n",
    "endpoint = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    huggingfacehub_api_token=huggingface_token,\n",
    ")\n",
    "\n",
    "llm = ChatHuggingFace(llm=endpoint)\n",
    "\n",
    "print(\"âœ… HuggingFace LLM Loaded!\")\n",
    "print(f\"   Model: HuggingFaceH4/zephyr-7b-beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d85728d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Test Response:\n",
      "AI, or artificial intelligence, refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, reasoning, and decision making. These systems can analyze large amounts of data, identify patterns, and make informed decisions based on that analysis without being explicitly programmed to do so. AI systems can also adapt and learn over time as they interact with their environments, improving their performance through experience. AI technologies include natural language processing, computer vision, and machine learning, which enable machines to recognize patterns and objects, understand and respond to human language, and make predictions or decisions based on that understanding. AI is quickly advancing in areas such as healthcare, finance, logistics, transportation, and manufacturing, where it can help to automate processes, optimize efficiency, and improve decision-making. While AI holds tremendous promise, it also raises important social and ethical concerns, as it has the potential to disrupt some traditional jobs and society at large, as well as create new ones. The field of AI is still in its infancy and there are ongoing debates about its impact on employment, privacy, and other issues, as well as the need for new forms of regulation and computing resources to ensure its responsible development and deployment.\n",
      "\n",
      "In technical terms, AI encompasses various subfields, including machine learning, deep learning, neural networks, and reinforcement learning, among others. In machine learning, algorithms are trained on vast amounts of data to learn from past behavior and make predictions or decisions without being explicitly programmed to do so. Deep learning uses artificial neural networks, inspired by the human brain, to learn from large amounts of data. Reinforcement learning involves trial and error and reward systems, allowing machines to learn through experience. AI is playing an increasingly important role in many areas, such as self-driving cars, virtual assistants like Siri and Alexa, and search and advertising. AI has the potential to revolutionize a wide variety of industries and fields, from agriculture to healthcare and education, and is expected to transform society in numerous ways, but also raises issues such as job displacement and the need for human oversight. As AI continues to advance, it will become increasingly common in everyday life, leading to debates about regulation, fairness, and data privacy. As AI becomes more sophisticated, it will be important to ensure responsible development and deployment of these technologies, and to understand how they can benefit society while mitigating potential harms.\n",
      "\n",
      "In simpler terms, AI is the ability of\n"
     ]
    }
   ],
   "source": [
    "# Test the LLM\n",
    "response = llm.invoke(\"What is AI?\")\n",
    "print(\"LLM Test Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28a77a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RAG Chain with LCEL\n",
    "simple_prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on the following context:\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3962239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3864011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# Format documents for the prompt\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"Format documents for insertion into prompt\"\"\"\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        formatted.append(f\"Document {i+1} (Source: {source}):\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87fc252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | simple_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e7fbb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conversational RAG Chain\n",
    "\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use the provided context to answer questions.\"),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nQuestion: {input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b46574ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversational_rag():\n",
    "    \"\"\"Create a conversational RAG chain with memory\"\"\"\n",
    "    return (\n",
    "        RunnablePassthrough.assign(\n",
    "            context=lambda x: format_docs(retriever.invoke(x[\"input\"]))\n",
    "        )\n",
    "        | conversational_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "conversational_rag = create_conversational_rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63f5ea4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chains created successfully!\n",
      "Available chains:\n",
      "- simple_rag_chain: Basic Q&A\n",
      "- conversational_rag: Maintains conversation history\n",
      "- streaming_rag_chain: Supports token streaming\n"
     ]
    }
   ],
   "source": [
    "### Streaming RAG chain\n",
    "streaming_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | simple_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chains created successfully!\")\n",
    "print(\"Available chains:\")\n",
    "print(\"- simple_rag_chain: Basic Q&A\")\n",
    "print(\"- conversational_rag: Maintains conversation history\")\n",
    "print(\"- streaming_rag_chain: Supports token streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00651cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for different chain types\n",
    "def test_rag_chains(question: str):\n",
    "    \"\"\"Test all RAG chain variants\"\"\"\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Simple RAG\n",
    "    print(\"\\n1. Simple RAG Chain:\")\n",
    "    answer = simple_rag_chain.invoke(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "    # 2. Streaming RAG\n",
    "    print(\"\\n2. Streaming RAG:\")\n",
    "    print(\"Answer: \", end=\"\", flush=True)\n",
    "    for chunk in streaming_rag_chain.stream(question):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1ca87ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the difference between AI and machine learning\n",
      "================================================================================\n",
      "\n",
      "1. Simple RAG Chain:\n",
      "Answer: Based on the provided context, the difference between AI and machine learning is not directly addressed. Both terms are used independently in the given documents without any comparison or contrast being drawn between them. The first document discusses the use of AI for detecting fine-grained activities in newborn resuscitation videos, highlighting the challenges involved, while the second document proposes a generative AI framework based on Bayesian Neural Networks (BNNs) for a more generalized training objective. The third document introduces BNNs as a type of machine learning algorithm with biological neuron-inspired properties, such as high energy efficiency and the ability for synaptic plasticity, which is distinct from the traditional von Neumann architecture used in AI systems. Therefore, it seems that AI and machine learning are used interchangeably in this context.\n",
      "\n",
      "2. Streaming RAG:\n",
      "Answer: Based on the given context, the difference between AI and machine learning is not explicitly mentioned or implied. The first document discusses the challenge of recognizing fine-grained activities in detecting newborn resuscitation procedures using generative AI, while the second document proposes a framework using generative AI that focuses on the fundamental training objective and introduces a loss function to align the model's uncertainty estimates with human cognitive patterns. The third document introduces biologically-inspired neural networks, called Bayesian neural networks (BNNs), which are more power-efficient, sample-efficient, and display rich parallel dynamics compared to traditional AI systems. These networks have unique properties such as power efficiency and the ability for synaptic plasticity not present in traditional AI systems, and require less resources for training and stimulation compared to traditional AI systems. Therefore, the main difference seems to be the focus on biologically-inspired neural networks, specifically BNNs, in the third document, while the first and second documents focus on traditional AI approaches. The first document addresses a specific task of detecting activities in newborn resuscitation videos, and the second document proposes a framework for AI that is more generalizable and aligns with human cognitive patterns through uncertainty estimates. However, without further context, it is unclear how these documents compare or relate to the broader concepts of AI and machine learning.\n"
     ]
    }
   ],
   "source": [
    "test_rag_chains(\"What is the difference between AI and machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6e701f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Question: What is the difference between AI and Machine Learning?\n",
      "================================================================================\n",
      "\n",
      "1. Simple RAG Chain:\n",
      "Answer: Based solely on the provided context, the given question asks about the difference between artificial intelligence (AI) and generative AI (GenAI). While both are mentioned in the first document, the second and third documents introduce alternative approaches to learning and computation: biological neurons (BNNs) and their potential applications in synthetic biological intelligence and neurocomputing. Based on the context, it can be inferred that GenAI differs from traditional AI in its focus on detecting fine-grained activities in newborn resuscitation videos and the challenges in recognizing them, while BNNs (biological neurons) offer distinct benefits such as power efficiency, parallel dynamics, and robust functional connectivity that are not available to traditional AI systems. BNNs also require fewer resources and better align with human cognitive patterns through a proposed framework that explicitly connects predictive confidence and uncertainty estimates. Therefore, the difference lies in their underlying principles and approaches to learning and computation. While AI may rely on task-specific modifications for specific applications, GenAI may prioritize detecting fine-grained activities through generative methods, whereas BNNs leverage biological neurons and their unique properties for sample efficiency and cognitive pattern alignment.\n",
      "\n",
      "2. Streaming RAG:\n",
      "Answer: The context provided does not explicitly differentiate between AI and machine learning. Both terms, \"AI\" and \"machine learning,\" are used interchangeably in these sources from arXiv. However, AI is mentioned in the first document in relation to detecting specific activities in newborn resuscitation videos, while the second and third documents focus on generative AI (GenAI) and biological neurons (BNNs) that are more efficient and have different characteristics compared to traditional artificial intelligence systems. Therefore, it can be inferred that the context is discussing the potential of using generative AI, which encompasses machine learning, but it is not explicitly distinguishing between the two terms. Without further information, it is unclear if the sources are specifically discussing AI versus machine learning, as the terms are used interchangeably.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question: Explain deep learning in simple terms\n",
      "================================================================================\n",
      "\n",
      "1. Simple RAG Chain:\n",
      "Answer: Deep learning refers to a type of artificial intelligence that enables computers to learn and make predictions or classifications from large amounts of data, similar to how the human brain processes information. It has shown impressive accuracy in various medical imaging tasks like identifying diseases in X-rays, scans, and photos of skin lesions. However, some models used in medical imaging can sometimes be too confident in their predictions, even when they are wrong. This overconfidence can be a problem in critical applications, as it makes it difficult for doctors to determine when to trust the results. To address this issue, researchers have proposed two approaches: one is called Bayesian deep learning, which uses probabilistic neural networks (BNNs), inspired by the way biological neurons work in the brain, and another is called confidence-boundary loss (CUB-Loss) that penalizes models for being too sure about incorrect predictions and refines their uncertainty estimates. These methods help doctors understand when their predictions may be unreliable, making it easier to decide when to trust the results. BNNs have advantages over traditional deep learning methods, such as using less resources and being more flexible in their connections, as seen in biological neurons. Both approaches aim to improve the reliability of AI-assisted medical diagnosis.\n",
      "\n",
      "2. Streaming RAG:\n",
      "Answer: Deep learning is a technique in medical image analysis that allows for high accuracy in classification, similar to that of human experts, but the models it creates can sometimes be too confident in its predictions, leading to overconfidence in incorrect results. This can cause problems in critical healthcare applications because it's hard for doctors to know when to trust the results. To address this issue, a new framework called Confidence-Uncertainty Boundary Loss (CUB-Loss) is being developed to improve the models' reliability by penalizing high confidence errors and improving the accuracy of correct predictions. Additionally, a strategy called Dual Temperature Scaling is proposed to refine the models' uncertainty estimates after they're trained. This is important for medical decision-making because doctors need to know when predictions are uncertain to double-check them. Biological neural networks, which are more efficient and have unique features like parallel dynamics and plasticity, could potentially be used with them, unlike traditional artificial intelligence systems that require more resources. This is being explored in fields like Synthetic Biological Intelligence and Organoid Intelligence.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question: How does NLP work?\n",
      "================================================================================\n",
      "\n",
      "1. Simple RAG Chain:\n",
      "Answer: Based on the context provided, it seems that NLP (Natural Language Processing) techniques such as Self-supervised Language Models (SLM) are being improved in three different ways: \n",
      "\n",
      "1. LaCy, a pre-training method proposed in Document 1, which uses grammar parsing to decide which tokens to train and learn a special <CALL> token instead of predicting them, leading to improvements in downstream evaluations by prioritizing tokens with fewer factual errors due to their higher likelihood of having many acceptable continuations. This approach is beneficial as it improves the learning signal during pretraining and helps the SLM to know when to predict a <CALL> token and when to rely on its parametric knowledge to continue text.\n",
      "\n",
      "2. Text-based bridging, a plug-and-play alternative to joint pretraining, as presented in Document 2, uses MLLMs to produce intermediate textual chains known as Chain-of-Thoughts (CoTs) for precise spatial reasoning, but it suffers from a language bottleneck that drops fine-grained spatial structure, making it less effective for prompts requiring precise spatial reasoning.\n",
      "\n",
      "3. Text-to-SQL systems powered by Large Language Models (LLMs) show promise in accessing complex data sources such as Electronic Health Records (EHRs) through natural language queries, as described in Document 3. Retrieval-augmented approaches like RAGs have shown positive results.\n",
      "\n",
      "Overall, the provided context suggests that NLP techniques are being developed and applied to various tasks, including improving SLM pretraining, mitigating computational expense and data requirements, and enabling NLP queries for accessing complex data sources like EHRs through natural language. However, some text-based bridging methods incur a language bottleneck for prompts requiring fine-grained spatial reasoning.\n",
      "\n",
      "2. Streaming RAG:\n",
      "Answer: Based on the given context, NLP (Natural Language Processing) techniques, such as transformer-based models like Large Language Models (LLMs), can be trained using a novel method called LaCy that combines grammar parsing and a loss function to decide which tokens to learn and which to predict as <CALL> (placeholders for further reasoning in text generation tasks. This approach improves the learning signal during pretraining by prioritizing tokens with fewer factual errors due to their higher likelihood of having multiple acceptable continuations. Text-based bridging, which uses MLLMs to produce intermediate textual representations called chain-of-thoughts (CoTs, can be an alternative, but it can be computationally expensive and data-hungry, requiring joint pretraining. It suffers from a language bottleneck when translating spatial constraints into natural language, leading to failures in prompts that require precise spatial reasoning. Text-to-SQL systems, powered by LLMs, can facilitate access to complex data sources like Electronic Health Records (EHRs) through natural language queries, but retrieval-augmented approaches like RAG have shown promising results. However, LLMs require joint pretraining, making them computationally expensive and data-hungry. This is in contrast to LaCy, which is a simpler and more flexible alternative that consistently improves the learning signal during pretraining by deciding when to predict a <CALL> token and when to rely on parametric knowledge to continue text generation. This information suggests that LLMs can be trained more efficiently with LaCy's grammar parsing and loss function to learn which tokens to prioritize for better accuracy and fewer factual errors in text generation tasks, while text-based bridging alternatives like CoTs may have limitations in preserving fine-grained spatial structure due to the language bottleneck. The text-based bridge method can be a plug-and-play alternative, but it can have limitations in translating precise spatial constraints into natural language, making it less effective for prompts requiring precise spatial reasoning.\n"
     ]
    }
   ],
   "source": [
    "# Test with multiple questions\n",
    "test_questions = [\n",
    "    \"What is the difference between AI and Machine Learning?\",\n",
    "    \"Explain deep learning in simple terms\",\n",
    "    \"How does NLP work?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    test_rag_chains(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5624b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversational RAG Example:\n",
      "Q1: What is machine learning?\n",
      "A1: Machine learning is the field of study focused on developing algorithms and models that allow computers to learn and make predictions or decisions based on data without being explicitly programmed to do so. It involves techniques such as deep learning, which uses neural networks with many layers to learn patterns and features from large datasets and can achieve human-level performance in tasks such as image and speech recognition, as well as natural language processing. In the context provided, three documents discuss different aspects of this field. Document 1 mentions the use of deterministic deep neural networks for medical image analysis, which have demonstrated high accuracy but can suffer from overconfidence in their predictions, making it difficult for healthcare professionals to determine when to trust the model's output. Document 2 discusses the need for predicting aggregate statistics for a population based on data points and the importance of understanding the uncertainty or confidence level of those predictions. Document 3 proposes a framework called Confidence-Uncertainty Boundary Curve (CUBC) to establish a relationship between a model's uncertainty estimates and human cognitive patterns for more reliable decision making. Overall, the documents highlight the importance of understanding uncertainty and confidence in machine learning models, beyond just high accuracy in specific tasks.\n"
     ]
    }
   ],
   "source": [
    "## Conversational example\n",
    "print(\"Conversational RAG Example:\")\n",
    "chat_history = []\n",
    "\n",
    "# First question\n",
    "q1 = \"What is machine learning?\"\n",
    "a1 = conversational_rag.invoke({\n",
    "    \"input\": q1,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "print(f\"Q1: {q1}\")\n",
    "print(f\"A1: {a1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c768648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update history\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=q1),\n",
    "    AIMessage(content=a1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dab2ff18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q2: How is it different from traditional programming?\n",
      "A2: The provided context doesn't directly compare machine learning to traditional programming, but it mentions the potential for large language models (LLMs) to serve as interactive curriculum designers for embodied AI systems and the need for understanding uncertainty and confidence in machine learning models, as discussed in Document 1 and 2. However, in Document 3, it introduces a new approach called Spatially-Aware Generation (SAGen) that combines a Forward model (FM) with an MLLM-based planner for precise constraint enforcement and explicit coordinate specification for layout planning, offering advantages such as efficiency and strong reliability in complex spatial prompts, which differs from traditional programming in that it is a model-based approach for generating layout plans instead of traditional programming that follows explicit instructions.\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question\n",
    "q2 = \"How is it different from traditional programming?\"\n",
    "a2 = conversational_rag.invoke({\n",
    "    \"input\": q2,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "print(f\"\\nQ2: {q2}\")\n",
    "print(f\"A2: {a2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
