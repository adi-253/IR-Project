{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffccc149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d630d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 chunks from ArXiv papers\n",
      "\\ Sample Chunk:\n",
      "   Title: Agentic Test-Time Scaling for WebAgents\n",
      "   Section: Abstract\n",
      "   Content preview: Test-time scaling has become a standard way\n",
      "to improve performance and boost reliability of\n",
      "neural network models. However, its behavior\n",
      "on agentic, multi-step tasks remains less well-\n",
      "understood: sma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:28: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/tmp/ipykernel_6286/2907912306.py:28: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(f\"\\ Sample Chunk:\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to chunked data - Load at the beginning\n",
    "chunked_data_path = Path(\"../../data-ingestion/processed/chunked/hybrid/\")\n",
    "\n",
    "# Load a few chunked documents\n",
    "def load_chunked_documents(path, max_files=5):\n",
    "    \"\"\"Load chunked documents from JSON files\"\"\"\n",
    "    documents = []\n",
    "    json_files = list(path.glob(\"*.json\"))[:max_files]\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r') as f:\n",
    "            chunks = json.load(f)\n",
    "            for chunk in chunks[:3]:  # Take first 3 chunks from each file\n",
    "                documents.append({\n",
    "                    'content': chunk['content'],\n",
    "                    'metadata': chunk['metadata']\n",
    "                })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load sample documents at the start\n",
    "chunked_docs = load_chunked_documents(chunked_data_path, max_files=3)\n",
    "print(f\"Loaded {len(chunked_docs)} chunks from ArXiv papers\")\n",
    "print(f\"\\ Sample Chunk:\")\n",
    "print(f\"   Title: {chunked_docs[0]['metadata']['title']}\")\n",
    "print(f\"   Section: {chunked_docs[0]['metadata']['section']}\")\n",
    "print(f\"   Content preview: {chunked_docs[0]['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc6d2406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Overview:\n",
      "\n",
      "Total chunks loaded: 9\n",
      "Chunking strategy: hybrid\n",
      "\n",
      "Sample Papers:\n",
      "   1. [2602.12276v1] Agentic Test-Time Scaling for WebAgents...\n",
      "   2. [2602.12251v1] A technical curriculum on language-oriented artificial intelligence in...\n",
      "   3. [2602.11322v1] Predictive Associative Memory: Retrieval Beyond Similarity Through Tem...\n",
      "\n",
      "These chunks will be used in all examples below!\n"
     ]
    }
   ],
   "source": [
    "# Show examples of the data we're working with\n",
    "print(\"Data Overview:\\n\")\n",
    "print(f\"Total chunks loaded: {len(chunked_docs)}\")\n",
    "print(f\"Chunking strategy: {chunked_docs[0]['metadata']['chunk_type']}\")\n",
    "print(f\"\\nSample Papers:\")\n",
    "\n",
    "unique_papers = {}\n",
    "for doc in chunked_docs:\n",
    "    paper_id = doc['metadata']['arxiv_id']\n",
    "    if paper_id not in unique_papers:\n",
    "        unique_papers[paper_id] = doc['metadata']['title']\n",
    "\n",
    "for i, (arxiv_id, title) in enumerate(list(unique_papers.items())[:3], 1):\n",
    "    print(f\"   {i}. [{arxiv_id}] {title[:70]}...\")\n",
    "\n",
    "print(f\"\\nThese chunks will be used in all examples below!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4cf28cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adi/Desktop/Projects/agentic-rag/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Huggingface And OpenAI Models\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "## Initialize a simple Embedding model(no API Key needed!)\n",
    "embeddings=HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1631d2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: Agentic Test-Time Scaling for WebAgents...\n",
      "Section: Abstract\n",
      "\n",
      "Text preview: Test-time scaling has become a standard way\n",
      "to improve performance and boost reliability of\n",
      "neural network models. However, its behavior\n",
      "on agentic, m...\n",
      "\n",
      "Embedding length: 384\n",
      "Sample values: [-0.0693819522857666, -0.11218364536762238, -0.06601215898990631, 0.047143902629613876, 0.020564280450344086]\n"
     ]
    }
   ],
   "source": [
    "## Create embeddings from real research paper chunk\n",
    "text = chunked_docs[0]['content']\n",
    "\n",
    "embedding = embeddings.embed_query(text)\n",
    "print(f\"Paper: {chunked_docs[0]['metadata']['title'][:60]}...\")\n",
    "print(f\"Section: {chunked_docs[0]['metadata']['section']}\")\n",
    "print(f\"\\nText preview: {text[:150]}...\")\n",
    "print(f\"\\nEmbedding length: {len(embedding)}\")\n",
    "print(f\"Sample values: {embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6cef7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 embeddings from research paper chunks\n",
      "Each embedding has 384 dimensions\n",
      "\n",
      "First chunk from: Agentic Test-Time Scaling for WebAgents...\n",
      "Second chunk from: Agentic Test-Time Scaling for WebAgents...\n",
      "\n",
      "‚úÖ Embedding shape: (5, 384)\n"
     ]
    }
   ],
   "source": [
    "# Use actual research paper chunks instead of simple sentences\n",
    "paper_texts = [doc['content'] for doc in chunked_docs[:5]]\n",
    "\n",
    "embedding_sentence = embeddings.embed_documents(paper_texts)\n",
    "\n",
    "print(f\"Created {len(embedding_sentence)} embeddings from research paper chunks\")\n",
    "print(f\"Each embedding has {len(embedding_sentence[0])} dimensions\")\n",
    "print(f\"\\nFirst chunk from: {chunked_docs[0]['metadata']['title'][:50]}...\")\n",
    "print(f\"Second chunk from: {chunked_docs[1]['metadata']['title'][:50]}...\")\n",
    "\n",
    "# Show that identical texts have identical embeddings\n",
    "print(f\"\\nEmbedding shape: ({len(embedding_sentence)}, {len(embedding_sentence[0])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f9ab52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Measuring Similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Cosine similarity measures the angle between two vectors.\n",
    "    - Result close to 1: Very similar\n",
    "    - Result close to 0: Not related\n",
    "    - Result close to -1: Opposite meanings\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_a = np.linalg.norm(vec1)\n",
    "    norm_b = np.linalg.norm(vec2)\n",
    "    \n",
    "    # Handle edge case: zero vectors\n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c347af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Chunk 1: Abstract from Agentic Test-Time Scaling for WebAgents...\n",
      "üìÑ Chunk 2: 1. Introduction from Agentic Test-Time Scaling for WebAgents...\n",
      "üìÑ Chunk 3: 1. Introduction from Agentic Test-Time Scaling for WebAgents...\n",
      "\n",
      "üî¢ Similarity (Chunk 1 vs Chunk 2): 0.4222\n",
      "üî¢ Similarity (Chunk 1 vs Chunk 3): 0.5202\n"
     ]
    }
   ],
   "source": [
    "# Example: Compare similarity between actual research paper chunks\n",
    "# Use embeddings from real chunks\n",
    "chunk1_embedding = embedding_sentence[0]\n",
    "chunk2_embedding = embedding_sentence[1]\n",
    "chunk3_embedding = embedding_sentence[2]\n",
    "\n",
    "similarity_1_2 = cosine_similarity(chunk1_embedding, chunk2_embedding)\n",
    "similarity_1_3 = cosine_similarity(chunk1_embedding, chunk3_embedding)\n",
    "\n",
    "print(f\"Chunk 1: {chunked_docs[0]['metadata']['section']} from {chunked_docs[0]['metadata']['title'][:40]}...\")\n",
    "print(f\"Chunk 2: {chunked_docs[1]['metadata']['section']} from {chunked_docs[1]['metadata']['title'][:40]}...\")\n",
    "print(f\"Chunk 3: {chunked_docs[2]['metadata']['section']} from {chunked_docs[2]['metadata']['title'][:40]}...\")\n",
    "print(f\"\\nSimilarity (Chunk 1 vs Chunk 2): {similarity_1_2:.4f}\")\n",
    "print(f\"Similarity (Chunk 1 vs Chunk 3): {similarity_1_3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf30841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Similarity (Chunk 2 vs Chunk 3): 0.4739\n",
      "\n",
      "üí° Higher scores = more semantically similar content\n",
      "üí° Chunks from same paper/topic typically have higher similarity\n"
     ]
    }
   ],
   "source": [
    "# Compare chunks from different papers\n",
    "similarity_2_3 = cosine_similarity(chunk2_embedding, chunk3_embedding)\n",
    "print(f\"Similarity (Chunk 2 vs Chunk 3): {similarity_2_3:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602aa26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Similarity Matrix between Paper Chunks:\n",
      "\n",
      "        Chunk 1 Chunk 2 Chunk 3 Chunk 4 Chunk 5\n",
      "Chunk 1   1.000   0.422   0.520   0.233   0.226\n",
      "Chunk 2   0.422   1.000   0.474   0.379   0.456\n",
      "Chunk 3   0.520   0.474   1.000   0.220   0.254\n",
      "Chunk 4   0.233   0.379   0.220   1.000   0.533\n",
      "Chunk 5   0.226   0.456   0.254   0.533   1.000\n",
      "\n",
      "üí° Diagonal values are 1.0 (each chunk is identical to itself)\n",
      "üí° Higher values indicate more semantic similarity between chunks\n"
     ]
    }
   ],
   "source": [
    "# Compare similarity between different paper chunks\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Similarity Matrix between Paper Chunks:\\n\")\n",
    "\n",
    "# Ensure required variables are available\n",
    "if \"paper_chunks\" not in globals():\n",
    "    paper_chunks = [doc[\"content\"] for doc in chunked_docs]\n",
    "if \"paper_embeddings\" not in globals():\n",
    "    paper_embeddings = embeddings.embed_documents(paper_chunks)\n",
    "\n",
    "# Create a small similarity matrix for first 5 chunks\n",
    "n_chunks = min(5, len(paper_chunks))\n",
    "similarity_matrix = []\n",
    "\n",
    "for i in range(n_chunks):\n",
    "    row = []\n",
    "    for j in range(n_chunks):\n",
    "        sim = cosine_similarity(paper_embeddings[i], paper_embeddings[j])\n",
    "        row.append(f\"{sim:.3f}\")\n",
    "    similarity_matrix.append(row)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "df = pd.DataFrame(\n",
    "    similarity_matrix, \n",
    "    columns=[f\"Chunk {i+1}\" for i in range(n_chunks)],\n",
    "    index=[f\"Chunk {i+1}\" for i in range(n_chunks)]\n",
    ")\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd2b2a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Search Query: 'What are neural networks and machine learning?'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "1. Score: 0.3288\n",
      "   Paper: A technical curriculum on language-oriented artificial intelligence in translation and specialised communication\n",
      "   Section: Abstract\n",
      "   ArXiv ID: 2602.12251v1\n",
      "   Content: This paper presents a technical curricu-\n",
      "lum on language-oriented artificial intel-\n",
      "ligence (AI) in the language and transla-\n",
      "tion (L&T) industry. The curriculum aims\n",
      "to foster domain-specific technic...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. Score: 0.2492\n",
      "   Paper: A technical curriculum on language-oriented artificial intelligence in translation and specialised communication\n",
      "   Section: 1\n",
      "Introduction\n",
      "   ArXiv ID: 2602.12251v1\n",
      "   Content: The recent emergence of general-purpose AI\n",
      "(GPAI) technologies in the form of large language\n",
      "¬© 2026 The author. This article is licensed under a Creative\n",
      "Commons 4.0 licence, no derivative works, attr...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. Score: 0.2269\n",
      "   Paper: A technical curriculum on language-oriented artificial intelligence in translation and specialised communication\n",
      "   Section: 1\n",
      "Introduction\n",
      "   ArXiv ID: 2602.12251v1\n",
      "   Content: ously beyond the scope of these technologies (e.g.,\n",
      "augmenting traditional automatic translation qual-\n",
      "ity control with meaning-based source- and target-\n",
      "segment comparisons) and they can (partially) ...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Semantic search on research papers\n",
    "def semantic_search_papers(query, documents, doc_metadata, embeddings_model, top_k=3):\n",
    "    \"\"\"Search through research paper chunks\"\"\"\n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "    doc_embeddings = embeddings_model.embed_documents(documents)\n",
    "    \n",
    "    similarities = []\n",
    "    for i, doc_emb in enumerate(doc_embeddings):\n",
    "        similarity = cosine_similarity(query_embedding, doc_emb)\n",
    "        similarities.append((similarity, documents[i], doc_metadata[i]))\n",
    "    \n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Test with a query\n",
    "query = \"What are neural networks and machine learning?\"\n",
    "results = semantic_search_papers(\n",
    "    query, \n",
    "    paper_chunks, \n",
    "    [doc['metadata'] for doc in chunked_docs],\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "print(f\"üîç Search Query: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "for i, (score, content, metadata) in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "    print(f\"   Paper: {metadata['title']}\")\n",
    "    print(f\"   Section: {metadata['section']}\")\n",
    "    print(f\"   ArXiv ID: {metadata['arxiv_id']}\")\n",
    "    print(f\"   Content: {content[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f712e87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings for 9 paper chunks\n",
      "Each embedding has 384 dimensions\n",
      "\n",
      "First chunk preview:\n",
      "Test-time scaling has become a standard way\n",
      "to improve performance and boost reliability of\n",
      "neural network models. However, its behavior\n",
      "on agentic, multi-step tasks remains less well-\n",
      "understood: small per-step errors can compound\n",
      "over long horizons; and we find that naive policies\n",
      "that uniformly i...\n"
     ]
    }
   ],
   "source": [
    "# Extract just the content from our chunked documents\n",
    "paper_chunks = [doc['content'] for doc in chunked_docs]\n",
    "\n",
    "# Create embeddings for the research paper chunks\n",
    "paper_embeddings = embeddings.embed_documents(paper_chunks)\n",
    "\n",
    "print(f\"Created embeddings for {len(paper_chunks)} paper chunks\")\n",
    "print(f\"Each embedding has {len(paper_embeddings[0])} dimensions\")\n",
    "print(f\"\\nFirst chunk preview:\")\n",
    "print(paper_chunks[0][:300] + \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
