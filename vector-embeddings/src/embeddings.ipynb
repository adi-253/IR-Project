{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce50322f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 chunks from ArXiv papers\n",
      "\n",
      "Sample Paper:\n",
      "   Title: Agentic Test-Time Scaling for WebAgents\n",
      "   Category: cs.AI\n",
      "   Section: Abstract\n",
      "   Chunk Type: hybrid\n",
      "\n",
      "   Content Preview:\n",
      "   Test-time scaling has become a standard way\n",
      "to improve performance and boost reliability of\n",
      "neural network models. However, its behavior\n",
      "on agentic, multi-step tasks remains less well-\n",
      "understood: small per-step errors can compound\n",
      "over long horizons...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Setup path to chunked data\n",
    "chunked_data_path = Path(\"../../data-ingestion/processed/chunked/hybrid/\")\n",
    "\n",
    "# Load chunked documents from the data-ingestion pipeline\n",
    "def load_arxiv_chunks(path, max_files=5, chunks_per_file=5):\n",
    "    \"\"\"Load chunked ArXiv papers\"\"\"\n",
    "    documents = []\n",
    "    json_files = list(path.glob(\"*.json\"))[:max_files]\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r') as f:\n",
    "            chunks = json.load(f)\n",
    "            for chunk in chunks[:chunks_per_file]:\n",
    "                documents.append({\n",
    "                    'content': chunk['content'],\n",
    "                    'metadata': chunk['metadata']\n",
    "                })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load papers early for use throughout the notebook\n",
    "arxiv_chunks = load_arxiv_chunks(chunked_data_path, max_files=5)\n",
    "\n",
    "print(f\"Loaded {len(arxiv_chunks)} chunks from ArXiv papers\")\n",
    "print(f\"\\nSample Paper:\")\n",
    "print(f\"   Title: {arxiv_chunks[0]['metadata']['title']}\")\n",
    "print(f\"   Category: {arxiv_chunks[0]['metadata']['primary_category']}\")\n",
    "print(f\"   Section: {arxiv_chunks[0]['metadata']['section']}\")\n",
    "print(f\"   Chunk Type: {arxiv_chunks[0]['metadata']['chunk_type']}\")\n",
    "print(f\"\\n   Content Preview:\")\n",
    "print(f\"   {arxiv_chunks[0]['content'][:250]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e61761cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "Data Overview for This Notebook:\n",
      "\n",
      "5 research papers:\n",
      "\n",
      "   1. [2602.12276v1] cs.AI\n",
      "      Agentic Test-Time Scaling for WebAgents...\n",
      "\n",
      "   2. [2602.12251v1] cs.CL\n",
      "      A technical curriculum on language-oriented artificial intelligence in tran...\n",
      "\n",
      "   3. [2602.11322v1] cs.LG\n",
      "      Predictive Associative Memory: Retrieval Beyond Similarity Through Temporal...\n",
      "\n",
      "   4. [2602.12236v1] cs.NE\n",
      "      Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Netwo...\n",
      "\n",
      "   5. [2602.11947v1] math.OC\n",
      "      Mixed-Integer Programming for Change-point Detection...\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display data overview\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"Data Overview for This Notebook:\\n\")\n",
    "\n",
    "unique_papers = {}\n",
    "for doc in arxiv_chunks:\n",
    "    paper_id = doc['metadata']['arxiv_id']\n",
    "    if paper_id not in unique_papers:\n",
    "        unique_papers[paper_id] = {\n",
    "            'title': doc['metadata']['title'],\n",
    "            'category': doc['metadata']['primary_category']\n",
    "        }\n",
    "\n",
    "print(f\"{len(unique_papers)} research papers:\")\n",
    "for i, (arxiv_id, info) in enumerate(list(unique_papers.items())[:5], 1):\n",
    "    print(f\"\\n   {i}. [{arxiv_id}] {info['category']}\")\n",
    "    print(f\"      {info['title'][:75]}...\")\n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "21d51f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace Embeddings Model Loaded!\n",
      "Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Embedding Dimension: 384\n"
     ]
    }
   ],
   "source": [
    "## Import HuggingFace Embeddings (No API Key Required!)\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embeddings model - runs locally without API keys\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "print(\"HuggingFace Embeddings Model Loaded!\")\n",
    "print(f\"Model: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(f\"Embedding Dimension: 384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "064f6c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Text Embedding from ArXiv Paper:\n",
      "Paper: Agentic Test-Time Scaling for WebAgents...\n",
      "Section: Abstract\n",
      "\n",
      "Input preview: Test-time scaling has become a standard way\n",
      "to improve performance and boost reliability of\n",
      "neural network models. However, its behavior\n",
      "on agentic, m...\n",
      "\n",
      " Output: Vector of 384 dimensions\n",
      "Sample values: [-0.0693819522857666, -0.11218364536762238, -0.06601215898990631, 0.047143902629613876, 0.020564280450344086]\n"
     ]
    }
   ],
   "source": [
    "## Single text embeddings - using real research paper chunk\n",
    "single_text = arxiv_chunks[0]['content']\n",
    "single_embeddings = embeddings.embed_query(single_text)\n",
    "\n",
    "print(\"Single Text Embedding from ArXiv Paper:\")\n",
    "print(f\"Paper: {arxiv_chunks[0]['metadata']['title'][:60]}...\")\n",
    "print(f\"Section: {arxiv_chunks[0]['metadata']['section']}\")\n",
    "print(f\"\\nInput preview: {single_text[:150]}...\")\n",
    "print(f\"\\n Output: Vector of {len(single_embeddings)} dimensions\")\n",
    "print(f\"Sample values: {single_embeddings[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5aca928a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real research paper chunks:\n",
      "1. Agentic Test-Time Scaling for WebAgents... (Section: Abstract)\n",
      "2. Agentic Test-Time Scaling for WebAgents... (Section: 1. Introduction)\n",
      "3. Agentic Test-Time Scaling for WebAgents... (Section: 1. Introduction)\n",
      "4. Agentic Test-Time Scaling for WebAgents... (Section: 1. Introduction)\n"
     ]
    }
   ],
   "source": [
    "# Multiple texts at once - using real research paper chunks\n",
    "multiple_texts = [doc['content'] for doc in arxiv_chunks[:4]]\n",
    "\n",
    "print(\"Using real research paper chunks:\")\n",
    "for i, doc in enumerate(arxiv_chunks[:4], 1):\n",
    "    print(f\"{i}. {doc['metadata']['title'][:50]}... (Section: {doc['metadata']['section']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1bb97ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multiple Text Embeddings:\n",
      "Number of chunks: 4\n",
      "Number of embeddings: 4\n",
      "Each embedding size: 384 dimensions\n",
      "\n",
      "First embedding sample: [-0.0693819671869278, -0.11218366026878357, -0.06601212918758392, 0.04714391380548477, 0.020564256235957146]\n",
      "\n",
      "Each research paper chunk is now represented as a 384-dimensional vector\n"
     ]
    }
   ],
   "source": [
    "multiple_embeddings = embeddings.embed_documents(multiple_texts)\n",
    "\n",
    "print(\"\\nMultiple Text Embeddings:\")\n",
    "print(f\"Number of chunks: {len(multiple_texts)}\")\n",
    "print(f\"Number of embeddings: {len(multiple_embeddings)}\")\n",
    "print(f\"Each embedding size: {len(multiple_embeddings[0])} dimensions\")\n",
    "print(f\"\\nFirst embedding sample: {multiple_embeddings[0][:5]}\")\n",
    "print(f\"\\nEach research paper chunk is now represented as a {len(multiple_embeddings[0])}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6dd8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cosine Similarity With OpenAI Embeddings\n",
    "\n",
    "import numpy as np\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Cosine similarity measures the angle between two vectors.\n",
    "    - Result close to 1: Very similar\n",
    "    - Result close to 0: Not related\n",
    "    - Result close to -1: Opposite meanings\n",
    "    \"\"\"\n",
    "\n",
    "    dot_product=np.dot(vec1,vec2)\n",
    "    norm_a=np.linalg.norm(vec1)\n",
    "    norm_b=np.linalg.norm(vec2)\n",
    "    return dot_product/(norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5075e33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing similarity between these research paper chunks:\n",
      "\n",
      "1. Paper: Agentic Test-Time Scaling for WebAgents...\n",
      "   Section: Abstract\n",
      "\n",
      "2. Paper: Agentic Test-Time Scaling for WebAgents...\n",
      "   Section: 1. Introduction\n",
      "\n",
      "3. Paper: Agentic Test-Time Scaling for WebAgents...\n",
      "   Section: 1. Introduction\n",
      "\n",
      "4. Paper: Agentic Test-Time Scaling for WebAgents...\n",
      "   Section: 1. Introduction\n",
      "\n",
      "5. Paper: Agentic Test-Time Scaling for WebAgents...\n",
      "   Section: 1. Introduction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding similar chunks - using real research paper chunks\n",
    "sentences = [doc['content'] for doc in arxiv_chunks[:5]]\n",
    "\n",
    "print(\"Analyzing similarity between these research paper chunks:\\n\")\n",
    "for i, doc in enumerate(arxiv_chunks[:5], 1):\n",
    "    print(f\"{i}. Paper: {doc['metadata']['title'][:45]}...\")\n",
    "    print(f\"   Section: {doc['metadata']['section']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19fadacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings=embeddings.embed_documents(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6dfbce74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise Similarity Scores:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Chunk 1 (Abstract) vs Chunk 2 (1. Introduction)\n",
      "   Paper 1: Agentic Test-Time Scaling for WebAgents...\n",
      "   Paper 2: Agentic Test-Time Scaling for WebAgents...\n",
      "   Similarity Score: 0.4222\n",
      "\n",
      "Chunk 1 (Abstract) vs Chunk 3 (1. Introduction)\n",
      "   Paper 1: Agentic Test-Time Scaling for WebAgents...\n",
      "   Paper 2: Agentic Test-Time Scaling for WebAgents...\n",
      "   Similarity Score: 0.5202\n",
      "\n",
      "Chunk 1 (Abstract) vs Chunk 4 (1. Introduction)\n",
      "   Paper 1: Agentic Test-Time Scaling for WebAgents...\n",
      "   Paper 2: Agentic Test-Time Scaling for WebAgents...\n",
      "   Similarity Score: 0.5384\n",
      "\n",
      "Chunk 2 (1. Introduction) vs Chunk 3 (1. Introduction)\n",
      "   Paper 1: Agentic Test-Time Scaling for WebAgents...\n",
      "   Paper 2: Agentic Test-Time Scaling for WebAgents...\n",
      "   Similarity Score: 0.4739\n",
      "\n",
      "Chunk 2 (1. Introduction) vs Chunk 4 (1. Introduction)\n",
      "   Paper 1: Agentic Test-Time Scaling for WebAgents...\n",
      "   Paper 2: Agentic Test-Time Scaling for WebAgents...\n",
      "   Similarity Score: 0.4944\n",
      "\n",
      "Chunk 3 (1. Introduction) vs Chunk 4 (1. Introduction)\n",
      "   Paper 1: Agentic Test-Time Scaling for WebAgents...\n",
      "   Paper 2: Agentic Test-Time Scaling for WebAgents...\n",
      "   Similarity Score: 0.5996\n"
     ]
    }
   ],
   "source": [
    "## Calculate similarity between all pairs of research chunks\n",
    "\n",
    "print(\"Pairwise Similarity Scores:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(min(3, len(sentences))):  # Limit to first 3 for readability\n",
    "    for j in range(i+1, min(4, len(sentences))):\n",
    "        similarity = cosine_similarity(sentence_embeddings[i], sentence_embeddings[j])\n",
    "        \n",
    "        print(f\"\\nChunk {i+1} ({arxiv_chunks[i]['metadata']['section']}) vs Chunk {j+1} ({arxiv_chunks[j]['metadata']['section']})\")\n",
    "        print(f\"   Paper 1: {arxiv_chunks[i]['metadata']['title'][:40]}...\")\n",
    "        print(f\"   Paper 2: {arxiv_chunks[j]['metadata']['title'][:40]}...\")\n",
    "        print(f\"   Similarity Score: {similarity:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a02e3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example- Semantic Search- Retireve the similar sentence\n",
    "def semantic_search(query,documents,embeddings_models,top_k=3):\n",
    "    \"\"\"Simple semantic search implementation\"\"\"\n",
    "\n",
    "    ## embed query and doument\n",
    "\n",
    "    query_embedding=embeddings_models.embed_query(query)\n",
    "    doc_embeddings = embeddings_models.embed_documents(documents)\n",
    "\n",
    "    ## Calculate the similarity score\n",
    "\n",
    "    similarties=[]\n",
    "\n",
    "    for i,doc_emb in enumerate(doc_embeddings):\n",
    "        similarity=cosine_similarity(query_embedding,doc_emb)\n",
    "        similarties.append((similarity,documents[i]))\n",
    "\n",
    "    ## Sort by similarity\n",
    "    similarties.sort(reverse=True)\n",
    "    return similarties[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0ec04fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching through 10 research paper chunks\n",
      " Query: 'What are neural networks and machine learning?'\n",
      "\n",
      "Searching in papers:\n",
      "   1. Agentic Test-Time Scaling for WebAgents...\n",
      "   2. Agentic Test-Time Scaling for WebAgents...\n",
      "   3. Agentic Test-Time Scaling for WebAgents...\n",
      "   4. Agentic Test-Time Scaling for WebAgents...\n",
      "   5. Agentic Test-Time Scaling for WebAgents...\n",
      "   6. A technical curriculum on language-oriented artificial intel...\n",
      "   7. A technical curriculum on language-oriented artificial intel...\n",
      "   8. A technical curriculum on language-oriented artificial intel...\n",
      "   9. A technical curriculum on language-oriented artificial intel...\n",
      "   10. A technical curriculum on language-oriented artificial intel...\n"
     ]
    }
   ],
   "source": [
    "# Semantic search with real research papers\n",
    "documents = [doc['content'] for doc in arxiv_chunks[:10]]\n",
    "\n",
    "query = \"What are neural networks and machine learning?\"\n",
    "\n",
    "print(f\"Searching through {len(documents)} research paper chunks\")\n",
    "print(f\" Query: '{query}'\")\n",
    "print(f\"\\nSearching in papers:\")\n",
    "for i, doc in enumerate(arxiv_chunks[:10], 1):\n",
    "    print(f\"   {i}. {doc['metadata']['title'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d2ab558",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=semantic_search(query,documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f965c2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic Search Results for: 'What are neural networks and machine learning?'\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "1. Similarity: 0.3308\n",
      "   Paper: A technical curriculum on language-oriented artificial intelligen...\n",
      "   Section: approach\n",
      "   ArXiv ID: 2602.12251v1\n",
      "   Content: in various ways.\n",
      "The curriculum proposed in this paper focuses on\n",
      "developing technical AI literacy, which involves\n",
      "knowl...\n",
      "\n",
      "2. Similarity: 0.3288\n",
      "   Paper: A technical curriculum on language-oriented artificial intelligen...\n",
      "   Section: Abstract\n",
      "   ArXiv ID: 2602.12251v1\n",
      "   Content: This paper presents a technical curricu-\n",
      "lum on language-oriented artificial intel-\n",
      "ligence (AI) in the language and tra...\n",
      "\n",
      "3. Similarity: 0.2492\n",
      "   Paper: A technical curriculum on language-oriented artificial intelligen...\n",
      "   Section: 1\n",
      "Introduction\n",
      "   ArXiv ID: 2602.12251v1\n",
      "   Content: The recent emergence of general-purpose AI\n",
      "(GPAI) technologies in the form of large language\n",
      "¬© 2026 The author. This art...\n",
      "\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSemantic Search Results for: '{query}'\\n\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for i, (score, doc) in enumerate(results, 1):\n",
    "    # Find matching metadata\n",
    "    idx = documents.index(doc)\n",
    "    metadata = arxiv_chunks[idx]['metadata']\n",
    "    \n",
    "    print(f\"\\n{i}. Similarity: {score:.4f}\")\n",
    "    print(f\"   Paper: {metadata['title'][:65]}...\")\n",
    "    print(f\"   Section: {metadata['section']}\")\n",
    "    print(f\"   ArXiv ID: {metadata['arxiv_id']}\")\n",
    "    print(f\"   Content: {doc[:120]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0895c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 chunks from hybrid strategy\n",
      "Loaded 0 chunks from semantic strategy\n",
      "Loaded 6 chunks from recursive strategy\n",
      "Loaded 6 chunks from token_based strategy\n",
      "\n",
      "Total strategies loaded: 4\n"
     ]
    }
   ],
   "source": [
    "# Load chunks from different strategies\n",
    "strategies = ['hybrid', 'semantic', 'recursive', 'token_based']\n",
    "strategy_chunks = {}\n",
    "\n",
    "for strategy in strategies:\n",
    "    strategy_path = Path(f\"../../data-ingestion/processed/chunked/{strategy}/\")\n",
    "    if strategy_path.exists():\n",
    "        chunks = load_arxiv_chunks(strategy_path, max_files=2, chunks_per_file=3)\n",
    "        strategy_chunks[strategy] = chunks\n",
    "        print(f\"Loaded {len(chunks)} chunks from {strategy} strategy\")\n",
    "\n",
    "print(f\"\\nTotal strategies loaded: {len(strategy_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "faaaba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping semantic - no chunks available\n",
      "Chunking Strategy Comparison:\n",
      "\n",
      "   Strategy  Num Chunks  Avg Length  Min Length  Max Length\n",
      "     hybrid           6         949         752        1416\n",
      "  recursive           6         987         980         996\n",
      "token_based           6        2060        2008        2123\n",
      "\n",
      "Different strategies create chunks of varying sizes\n",
      "This affects the granularity and context of embeddings\n"
     ]
    }
   ],
   "source": [
    "# Analyze characteristics of different chunking strategies\n",
    "import pandas as pd\n",
    "\n",
    "strategy_stats = []\n",
    "\n",
    "for strategy, chunks in strategy_chunks.items():\n",
    "    if not chunks:  # Skip empty chunk lists\n",
    "        print(f\"Skipping {strategy} - no chunks available\")\n",
    "        continue\n",
    "    \n",
    "    contents = [c['content'] for c in chunks]\n",
    "    if not contents:  # Skip if no valid content\n",
    "        print(f\"Skipping {strategy} - no content available\")\n",
    "        continue\n",
    "    \n",
    "    avg_length = np.mean([len(c) for c in contents])\n",
    "    max_length = max([len(c) for c in contents])\n",
    "    min_length = min([len(c) for c in contents])\n",
    "    \n",
    "    strategy_stats.append({\n",
    "        'Strategy': strategy,\n",
    "        'Num Chunks': len(chunks),\n",
    "        'Avg Length': int(avg_length),\n",
    "        'Min Length': int(min_length),\n",
    "        'Max Length': int(max_length)\n",
    "    })\n",
    "\n",
    "if strategy_stats:\n",
    "    stats_df = pd.DataFrame(strategy_stats)\n",
    "    print(\"Chunking Strategy Comparison:\\n\")\n",
    "    print(stats_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nDifferent strategies create chunks of varying sizes\")\n",
    "    print(\"This affects the granularity and context of embeddings\")\n",
    "else:\n",
    "    print(\"No strategy statistics available - all strategies had empty chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdba3a1",
   "metadata": {},
   "source": [
    "## Comparing Different Chunking Strategies\n",
    "\n",
    "Let's compare embeddings from different chunking strategies (hybrid, semantic, recursive, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "246cabae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings for 25 research paper chunks\n",
      "Each embedding: 384 dimensions\n",
      "Total size: ~37.50 KB\n",
      "\n",
      "Papers include:\n",
      "   1. Mixed-Integer Programming for Change-point Detection...\n",
      "   2. Agentic Test-Time Scaling for WebAgents...\n",
      "   3. Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural ...\n",
      "   4. Predictive Associative Memory: Retrieval Beyond Similarity Through Tem...\n",
      "   5. A technical curriculum on language-oriented artificial intelligence in...\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for all loaded research papers\n",
    "paper_contents = [doc['content'] for doc in arxiv_chunks]\n",
    "paper_embeddings = embeddings.embed_documents(paper_contents)\n",
    "\n",
    "print(f\"Created embeddings for {len(paper_embeddings)} research paper chunks\")\n",
    "print(f\"Each embedding: {len(paper_embeddings[0])} dimensions\")\n",
    "print(f\"Total size: ~{len(paper_embeddings) * len(paper_embeddings[0]) * 4 / 1024:.2f} KB\")\n",
    "print(f\"\\nPapers include:\")\n",
    "unique_titles = set([doc['metadata']['title'] for doc in arxiv_chunks])\n",
    "for i, title in enumerate(list(unique_titles)[:5], 1):\n",
    "    print(f\"   {i}. {title[:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b508e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "Searching for: 'neural networks and deep learning'\n",
      "Searching through 25 chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Top 3 Results:\n",
      "\n",
      "1. Similarity: 0.3478\n",
      "   Paper: Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural ...\n",
      "   Section: 1\n",
      "Introduction\n",
      "   ArXiv ID: 2602.12236v1\n",
      "   Preview: Neuromorphic computing has emerged as a paradigm shift in artificial intelligence, drawing\n",
      "inspiration from the energy efficiency and temporal dynamic...\n",
      "\n",
      "2. Similarity: 0.3459\n",
      "   Paper: A technical curriculum on language-oriented artificial intelligence in...\n",
      "   Section: approach\n",
      "   ArXiv ID: 2602.12251v1\n",
      "   Preview: in various ways.\n",
      "The curriculum proposed in this paper focuses on\n",
      "developing technical AI literacy, which involves\n",
      "knowledge of the basic operating pr...\n",
      "\n",
      "3. Similarity: 0.3377\n",
      "   Paper: A technical curriculum on language-oriented artificial intelligence in...\n",
      "   Section: Abstract\n",
      "   ArXiv ID: 2602.12251v1\n",
      "   Preview: This paper presents a technical curricu-\n",
      "lum on language-oriented artificial intel-\n",
      "ligence (AI) in the language and transla-\n",
      "tion (L&T) industry. The...\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Searching for: 'machine learning algorithms'\n",
      "Searching through 25 chunks...\n",
      "\n",
      "üèÜ Top 3 Results:\n",
      "\n",
      "1. Similarity: 0.2998\n",
      "   Paper: Mixed-Integer Programming for Change-point Detection...\n",
      "   Section: experiments\n",
      "   ArXiv ID: 2602.11947v1\n",
      "   Preview: on benchmark\n",
      "real-world datasets demonstrate that the proposed formulations achieve reductions in solution\n",
      "times under both ‚Ñì1 and ‚Ñì2 loss functions i...\n",
      "\n",
      "2. Similarity: 0.2706\n",
      "   Paper: Predictive Associative Memory: Retrieval Beyond Similarity Through Tem...\n",
      "   Section: 1\n",
      "Introduction\n",
      "   ArXiv ID: 2602.11322v1\n",
      "   Preview: same temporal window, creating associative links be-\n",
      "tween experiences that may share no featural simi-\n",
      "larity. Complementary learning systems theory ...\n",
      "\n",
      "3. Similarity: 0.2681\n",
      "   Paper: Mixed-Integer Programming for Change-point Detection...\n",
      "   Section: 1\n",
      "Introduction\n",
      "   ArXiv ID: 2602.11947v1\n",
      "   Preview: downstream decision-making. We consider an offline version of this task, in which the objective is\n",
      "to retrospectively identify locations where the str...\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "Searching for: 'data processing and analysis'\n",
      "Searching through 25 chunks...\n",
      "\n",
      "üèÜ Top 3 Results:\n",
      "\n",
      "1. Similarity: 0.2852\n",
      "   Paper: Mixed-Integer Programming for Change-point Detection...\n",
      "   Section: experiments\n",
      "   ArXiv ID: 2602.11947v1\n",
      "   Preview: on benchmark\n",
      "real-world datasets demonstrate that the proposed formulations achieve reductions in solution\n",
      "times under both ‚Ñì1 and ‚Ñì2 loss functions i...\n",
      "\n",
      "2. Similarity: 0.2805\n",
      "   Paper: Mixed-Integer Programming for Change-point Detection...\n",
      "   Section: 1\n",
      "Introduction\n",
      "   ArXiv ID: 2602.11947v1\n",
      "   Preview: segment each data point belongs to. Therefore, by explicitly encoding segment assignments, MIP\n",
      "formulations allow precise control over the number of s...\n",
      "\n",
      "3. Similarity: 0.2495\n",
      "   Paper: Mixed-Integer Programming for Change-point Detection...\n",
      "   Section: 1\n",
      "Introduction\n",
      "   ArXiv ID: 2602.11947v1\n",
      "   Preview: Piecewise linear (PWL) fitting involves partitioning ordered observations into contiguous segments,\n",
      "each modeled by a linear function, with the segmen...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced semantic search with research papers\n",
    "def semantic_search_arxiv(query, documents, doc_metadata, embeddings_model, top_k=5):\n",
    "    \"\"\"Search through ArXiv paper chunks with detailed results\"\"\"\n",
    "    \n",
    "    print(f\"Searching for: '{query}'\")\n",
    "    print(f\"Searching through {len(documents)} chunks...\")\n",
    "    \n",
    "    # Create embeddings\n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "    doc_embeddings = embeddings_model.embed_documents(documents)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    def cosine_similarity(vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_a = np.linalg.norm(vec1)\n",
    "        norm_b = np.linalg.norm(vec2)\n",
    "        return dot_product / (norm_a * norm_b)\n",
    "    \n",
    "    similarities = []\n",
    "    for i, doc_emb in enumerate(doc_embeddings):\n",
    "        similarity = cosine_similarity(query_embedding, doc_emb)\n",
    "        similarities.append((similarity, documents[i], doc_metadata[i]))\n",
    "    \n",
    "    # Sort and return top results\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Test different queries\n",
    "test_queries = [\n",
    "    \"neural networks and deep learning\",\n",
    "    \"machine learning algorithms\",\n",
    "    \"data processing and analysis\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    results = semantic_search_arxiv(\n",
    "        query, \n",
    "        paper_contents, \n",
    "        [doc['metadata'] for doc in arxiv_chunks],\n",
    "        embeddings,\n",
    "        top_k=3\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüèÜ Top 3 Results:\\n\")\n",
    "    for i, (score, content, metadata) in enumerate(results, 1):\n",
    "        print(f\"{i}. Similarity: {score:.4f}\")\n",
    "        print(f\"   Paper: {metadata['title'][:70]}...\")\n",
    "        print(f\"   Section: {metadata['section']}\")\n",
    "        print(f\"   ArXiv ID: {metadata['arxiv_id']}\")\n",
    "        print(f\"   Preview: {content[:150]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "46e12d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic Search Results for: 'What is deep learning and neural network architecture?'\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "1. Similarity: 0.3148\n",
      "   Paper: A technical curriculum on language-oriented artificial intelligen...\n",
      "   Section: Abstract\n",
      "   Content: This paper presents a technical curricu-\n",
      "lum on language-oriented artificial intel-\n",
      "ligence (AI) in the language and tra...\n",
      "\n",
      "2. Similarity: 0.2969\n",
      "   Paper: A technical curriculum on language-oriented artificial intelligen...\n",
      "   Section: approach\n",
      "   Content: in various ways.\n",
      "The curriculum proposed in this paper focuses on\n",
      "developing technical AI literacy, which involves\n",
      "knowl...\n",
      "\n",
      "3. Similarity: 0.2521\n",
      "   Paper: A technical curriculum on language-oriented artificial intelligen...\n",
      "   Section: 1\n",
      "Introduction\n",
      "   Content: The recent emergence of general-purpose AI\n",
      "(GPAI) technologies in the form of large language\n",
      "¬© 2026 The author. This art...\n",
      "\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Another query on research papers\n",
    "query = \"What is deep learning and neural network architecture?\"\n",
    "results = semantic_search(query, documents, embeddings)\n",
    "\n",
    "print(f\"\\nSemantic Search Results for: '{query}'\\n\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for i, (score, doc) in enumerate(results, 1):\n",
    "    idx = documents.index(doc)\n",
    "    metadata = arxiv_chunks[idx]['metadata']\n",
    "    \n",
    "    print(f\"\\n{i}. Similarity: {score:.4f}\")\n",
    "    print(f\"   Paper: {metadata['title'][:65]}...\")\n",
    "    print(f\"   Section: {metadata['section']}\")\n",
    "    print(f\"   Content: {doc[:120]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
