{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce50322f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 chunks from ArXiv papers\n",
      "\n",
      "Sample Paper:\n",
      "   Title: Agentic Test-Time Scaling for WebAgents\n",
      "   Category: cs.AI\n",
      "   Section: Abstract\n",
      "   Chunk Type: hybrid\n",
      "\n",
      "   Content Preview:\n",
      "   Test-time scaling has become a standard way\n",
      "to improve performance and boost reliability of\n",
      "neural network models. However, its behavior\n",
      "on agentic, multi-step tasks remains less well-\n",
      "understood: small per-step errors can compound\n",
      "over long horizons...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Setup path to chunked data\n",
    "chunked_data_path = Path(\"../../data-ingestion/processed/chunked/hybrid/\")\n",
    "\n",
    "# Load chunked documents from the data-ingestion pipeline\n",
    "def load_arxiv_chunks(path, max_files=5, chunks_per_file=5):\n",
    "    \"\"\"Load chunked ArXiv papers\"\"\"\n",
    "    documents = []\n",
    "    json_files = list(path.glob(\"*.json\"))[:max_files]\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r') as f:\n",
    "            chunks = json.load(f)\n",
    "            for chunk in chunks[:chunks_per_file]:\n",
    "                documents.append({\n",
    "                    'content': chunk['content'],\n",
    "                    'metadata': chunk['metadata']\n",
    "                })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load papers early for use throughout the notebook\n",
    "arxiv_chunks = load_arxiv_chunks(chunked_data_path, max_files=5)\n",
    "\n",
    "print(f\"Loaded {len(arxiv_chunks)} chunks from ArXiv papers\")\n",
    "print(f\"\\nSample Paper:\")\n",
    "print(f\"   Title: {arxiv_chunks[0]['metadata']['title']}\")\n",
    "print(f\"   Category: {arxiv_chunks[0]['metadata']['primary_category']}\")\n",
    "print(f\"   Section: {arxiv_chunks[0]['metadata']['section']}\")\n",
    "print(f\"   Chunk Type: {arxiv_chunks[0]['metadata']['chunk_type']}\")\n",
    "print(f\"\\n   Content Preview:\")\n",
    "print(f\"   {arxiv_chunks[0]['content'][:250]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e61761cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "Data Overview for This Notebook:\n",
      "\n",
      "5 research papers:\n",
      "\n",
      "   1. [2602.12276v1] cs.AI\n",
      "      Agentic Test-Time Scaling for WebAgents...\n",
      "\n",
      "   2. [2602.12251v1] cs.CL\n",
      "      A technical curriculum on language-oriented artificial intelligence in tran...\n",
      "\n",
      "   3. [2602.11322v1] cs.LG\n",
      "      Predictive Associative Memory: Retrieval Beyond Similarity Through Temporal...\n",
      "\n",
      "   4. [2602.12236v1] cs.NE\n",
      "      Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Netwo...\n",
      "\n",
      "   5. [2602.11947v1] math.OC\n",
      "      Mixed-Integer Programming for Change-point Detection...\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display data overview\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"Data Overview for This Notebook:\\n\")\n",
    "\n",
    "unique_papers = {}\n",
    "for doc in arxiv_chunks:\n",
    "    paper_id = doc['metadata']['arxiv_id']\n",
    "    if paper_id not in unique_papers:\n",
    "        unique_papers[paper_id] = {\n",
    "            'title': doc['metadata']['title'],\n",
    "            'category': doc['metadata']['primary_category']\n",
    "        }\n",
    "\n",
    "print(f\"{len(unique_papers)} research papers:\")\n",
    "for i, (arxiv_id, info) in enumerate(list(unique_papers.items())[:5], 1):\n",
    "    print(f\"\\n   {i}. [{arxiv_id}] {info['category']}\")\n",
    "    print(f\"      {info['title'][:75]}...\")\n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21d51f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace Embeddings Model Loaded!\n",
      "Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Embedding Dimension: 384\n"
     ]
    }
   ],
   "source": [
    "## Import HuggingFace Embeddings (No API Key Required!)\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embeddings model - runs locally without API keys\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "print(\"HuggingFace Embeddings Model Loaded!\")\n",
    "print(f\"Model: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(f\"Embedding Dimension: 384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "064f6c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Single Text Embedding from ArXiv Paper:\n",
      "ğŸ“„ Paper: Agentic Test-Time Scaling for WebAgents...\n",
      "ğŸ“‚ Section: Abstract\n",
      "\n",
      "Input preview: Test-time scaling has become a standard way\n",
      "to improve performance and boost reliability of\n",
      "neural network models. However, its behavior\n",
      "on agentic, m...\n",
      "\n",
      "ğŸ“Š Output: Vector of 384 dimensions\n",
      "Sample values: [-0.0693819522857666, -0.11218364536762238, -0.06601215898990631, 0.047143902629613876, 0.020564280450344086]\n"
     ]
    }
   ],
   "source": [
    "## Single text embeddings - using real research paper chunk\n",
    "single_text = arxiv_chunks[0]['content']\n",
    "single_embeddings = embeddings.embed_query(single_text)\n",
    "\n",
    "print(\"ğŸ“ Single Text Embedding from ArXiv Paper:\")\n",
    "print(f\"ğŸ“„ Paper: {arxiv_chunks[0]['metadata']['title'][:60]}...\")\n",
    "print(f\"ğŸ“‚ Section: {arxiv_chunks[0]['metadata']['section']}\")\n",
    "print(f\"\\nInput preview: {single_text[:150]}...\")\n",
    "print(f\"\\nğŸ“Š Output: Vector of {len(single_embeddings)} dimensions\")\n",
    "print(f\"Sample values: {single_embeddings[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5aca928a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Using real research paper chunks:\n",
      "1. Agentic Test-Time Scaling for WebAgents... (Section: Abstract)\n",
      "2. Agentic Test-Time Scaling for WebAgents... (Section: 1. Introduction)\n",
      "3. Agentic Test-Time Scaling for WebAgents... (Section: 1. Introduction)\n",
      "4. Agentic Test-Time Scaling for WebAgents... (Section: 1. Introduction)\n"
     ]
    }
   ],
   "source": [
    "# Multiple texts at once - using real research paper chunks\n",
    "multiple_texts = [doc['content'] for doc in arxiv_chunks[:4]]\n",
    "\n",
    "print(\"ğŸ“š Using real research paper chunks:\")\n",
    "for i, doc in enumerate(arxiv_chunks[:4], 1):\n",
    "    print(f\"{i}. {doc['metadata']['title'][:50]}... (Section: {doc['metadata']['section']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1bb97ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Multiple Text Embeddings:\n",
      "Number of chunks: 4\n",
      "Number of embeddings: 4\n",
      "Each embedding size: 384 dimensions\n",
      "\n",
      "First embedding sample: [-0.0693819671869278, -0.11218366026878357, -0.06601212918758392, 0.04714391380548477, 0.020564256235957146]\n",
      "\n",
      "ğŸ’¡ Each research paper chunk is now represented as a 384-dimensional vector\n"
     ]
    }
   ],
   "source": [
    "multiple_embeddings = embeddings.embed_documents(multiple_texts)\n",
    "\n",
    "print(\"\\nğŸ“Š Multiple Text Embeddings:\")\n",
    "print(f\"Number of chunks: {len(multiple_texts)}\")\n",
    "print(f\"Number of embeddings: {len(multiple_embeddings)}\")\n",
    "print(f\"Each embedding size: {len(multiple_embeddings[0])} dimensions\")\n",
    "print(f\"\\nFirst embedding sample: {multiple_embeddings[0][:5]}\")\n",
    "print(f\"\\nğŸ’¡ Each research paper chunk is now represented as a {len(multiple_embeddings[0])}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6dd8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cosine Similarity With OpenAI Embeddings\n",
    "\n",
    "import numpy as np\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Cosine similarity measures the angle between two vectors.\n",
    "    - Result close to 1: Very similar\n",
    "    - Result close to 0: Not related\n",
    "    - Result close to -1: Opposite meanings\n",
    "    \"\"\"\n",
    "\n",
    "    dot_product=np.dot(vec1,vec2)\n",
    "    norm_a=np.linalg.norm(vec1)\n",
    "    norm_b=np.linalg.norm(vec2)\n",
    "    return dot_product/(norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5075e33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Analyzing similarity between these research paper chunks:\n",
      "\n",
      "1. Paper: Agentic Test-Time Scaling for WebAgents...\n",
      "   Section: Abstract\n",
      "\n",
      "2. Paper: Agentic Test-Time Scaling for WebAgents...\n",
      "   Section: 1. Introduction\n",
      "\n",
      "3. Paper: Agentic Test-Time Scaling for WebAgents...\n",
      "   Section: 1. Introduction\n",
      "\n",
      "4. Paper: Agentic Test-Time Scaling for WebAgents...\n",
      "   Section: 1. Introduction\n",
      "\n",
      "5. Paper: Agentic Test-Time Scaling for WebAgents...\n",
      "   Section: 1. Introduction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding similar chunks - using real research paper chunks\n",
    "sentences = [doc['content'] for doc in arxiv_chunks[:5]]\n",
    "\n",
    "print(\"ğŸ” Analyzing similarity between these research paper chunks:\\n\")\n",
    "for i, doc in enumerate(arxiv_chunks[:5], 1):\n",
    "    print(f\"{i}. Paper: {doc['metadata']['title'][:45]}...\")\n",
    "    print(f\"   Section: {doc['metadata']['section']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19fadacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings=embeddings.embed_documents(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dfbce74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Pairwise Similarity Scores:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ Chunk 1 (Abstract) vs Chunk 2 (1. Introduction)\n",
      "   Paper 1: Agentic Test-Time Scaling for WebAgents...\n",
      "   Paper 2: Agentic Test-Time Scaling for WebAgents...\n",
      "   Similarity Score: 0.4222\n",
      "\n",
      "ğŸ“„ Chunk 1 (Abstract) vs Chunk 3 (1. Introduction)\n",
      "   Paper 1: Agentic Test-Time Scaling for WebAgents...\n",
      "   Paper 2: Agentic Test-Time Scaling for WebAgents...\n",
      "   Similarity Score: 0.5202\n",
      "\n",
      "ğŸ“„ Chunk 1 (Abstract) vs Chunk 4 (1. Introduction)\n",
      "   Paper 1: Agentic Test-Time Scaling for WebAgents...\n",
      "   Paper 2: Agentic Test-Time Scaling for WebAgents...\n",
      "   Similarity Score: 0.5384\n",
      "\n",
      "ğŸ“„ Chunk 2 (1. Introduction) vs Chunk 3 (1. Introduction)\n",
      "   Paper 1: Agentic Test-Time Scaling for WebAgents...\n",
      "   Paper 2: Agentic Test-Time Scaling for WebAgents...\n",
      "   Similarity Score: 0.4739\n",
      "\n",
      "ğŸ“„ Chunk 2 (1. Introduction) vs Chunk 4 (1. Introduction)\n",
      "   Paper 1: Agentic Test-Time Scaling for WebAgents...\n",
      "   Paper 2: Agentic Test-Time Scaling for WebAgents...\n",
      "   Similarity Score: 0.4944\n",
      "\n",
      "ğŸ“„ Chunk 3 (1. Introduction) vs Chunk 4 (1. Introduction)\n",
      "   Paper 1: Agentic Test-Time Scaling for WebAgents...\n",
      "   Paper 2: Agentic Test-Time Scaling for WebAgents...\n",
      "   Similarity Score: 0.5996\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¡ Scores closer to 1.0 indicate more similar content\n",
      "ğŸ’¡ Chunks from similar topics/sections typically have higher scores\n"
     ]
    }
   ],
   "source": [
    "## Calculate similarity between all pairs of research chunks\n",
    "\n",
    "print(\"ğŸ“Š Pairwise Similarity Scores:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(min(3, len(sentences))):  # Limit to first 3 for readability\n",
    "    for j in range(i+1, min(4, len(sentences))):\n",
    "        similarity = cosine_similarity(sentence_embeddings[i], sentence_embeddings[j])\n",
    "        \n",
    "        print(f\"\\nğŸ“„ Chunk {i+1} ({arxiv_chunks[i]['metadata']['section']}) vs Chunk {j+1} ({arxiv_chunks[j]['metadata']['section']})\")\n",
    "        print(f\"   Paper 1: {arxiv_chunks[i]['metadata']['title'][:40]}...\")\n",
    "        print(f\"   Paper 2: {arxiv_chunks[j]['metadata']['title'][:40]}...\")\n",
    "        print(f\"   Similarity Score: {similarity:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ’¡ Scores closer to 1.0 indicate more similar content\")\n",
    "print(\"ğŸ’¡ Chunks from similar topics/sections typically have higher scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a02e3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example- Semantic Search- Retireve the similar sentence\n",
    "def semantic_search(query,documents,embeddings_models,top_k=3):\n",
    "    \"\"\"Simple semantic search implementation\"\"\"\n",
    "\n",
    "    ## embed query and doument\n",
    "\n",
    "    query_embedding=embeddings_models.embed_query(query)\n",
    "    doc_embeddings = embeddings_models.embed_documents(documents)\n",
    "\n",
    "    ## Calculate the similarity score\n",
    "\n",
    "    similarties=[]\n",
    "\n",
    "    for i,doc_emb in enumerate(doc_embeddings):\n",
    "        similarity=cosine_similarity(query_embedding,doc_emb)\n",
    "        similarties.append((similarity,documents[i]))\n",
    "\n",
    "    ## Sort by similarity\n",
    "    similarties.sort(reverse=True)\n",
    "    return similarties[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0ec04fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Searching through 10 research paper chunks\n",
      " Query: 'What are neural networks and machine learning?'\n",
      "\n",
      "ğŸ“š Searching in papers:\n",
      "   1. Agentic Test-Time Scaling for WebAgents...\n",
      "   2. Agentic Test-Time Scaling for WebAgents...\n",
      "   3. Agentic Test-Time Scaling for WebAgents...\n",
      "   4. Agentic Test-Time Scaling for WebAgents...\n",
      "   5. Agentic Test-Time Scaling for WebAgents...\n",
      "   6. A technical curriculum on language-oriented artificial intel...\n",
      "   7. A technical curriculum on language-oriented artificial intel...\n",
      "   8. A technical curriculum on language-oriented artificial intel...\n",
      "   9. A technical curriculum on language-oriented artificial intel...\n",
      "   10. A technical curriculum on language-oriented artificial intel...\n"
     ]
    }
   ],
   "source": [
    "# Semantic search with real research papers\n",
    "documents = [doc['content'] for doc in arxiv_chunks[:10]]\n",
    "\n",
    "query = \"What are neural networks and machine learning?\"\n",
    "\n",
    "print(f\"ğŸ” Searching through {len(documents)} research paper chunks\")\n",
    "print(f\" Query: '{query}'\")\n",
    "print(f\"\\nğŸ“š Searching in papers:\")\n",
    "for i, doc in enumerate(arxiv_chunks[:10], 1):\n",
    "    print(f\"   {i}. {doc['metadata']['title'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d2ab558",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=semantic_search(query,documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f965c2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Semantic Search Results for: 'What are neural networks and machine learning?'\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "1. Similarity: 0.3308\n",
      "   Paper: A technical curriculum on language-oriented artificial intelligen...\n",
      "   Section: approach\n",
      "   ArXiv ID: 2602.12251v1\n",
      "   Content: in various ways.\n",
      "The curriculum proposed in this paper focuses on\n",
      "developing technical AI literacy, which involves\n",
      "knowl...\n",
      "\n",
      "2. Similarity: 0.3288\n",
      "   Paper: A technical curriculum on language-oriented artificial intelligen...\n",
      "   Section: Abstract\n",
      "   ArXiv ID: 2602.12251v1\n",
      "   Content: This paper presents a technical curricu-\n",
      "lum on language-oriented artificial intel-\n",
      "ligence (AI) in the language and tra...\n",
      "\n",
      "3. Similarity: 0.2492\n",
      "   Paper: A technical curriculum on language-oriented artificial intelligen...\n",
      "   Section: 1\n",
      "Introduction\n",
      "   ArXiv ID: 2602.12251v1\n",
      "   Content: The recent emergence of general-purpose AI\n",
      "(GPAI) technologies in the form of large language\n",
      "Â© 2026 The author. This art...\n",
      "\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nğŸ” Semantic Search Results for: '{query}'\\n\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for i, (score, doc) in enumerate(results, 1):\n",
    "    # Find matching metadata\n",
    "    idx = documents.index(doc)\n",
    "    metadata = arxiv_chunks[idx]['metadata']\n",
    "    \n",
    "    print(f\"\\n{i}. Similarity: {score:.4f}\")\n",
    "    print(f\"   Paper: {metadata['title'][:65]}...\")\n",
    "    print(f\"   Section: {metadata['section']}\")\n",
    "    print(f\"   ArXiv ID: {metadata['arxiv_id']}\")\n",
    "    print(f\"   Content: {doc[:120]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0895c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 6 chunks from hybrid strategy\n",
      "âœ… Loaded 0 chunks from semantic strategy\n",
      "âœ… Loaded 6 chunks from recursive strategy\n",
      "âœ… Loaded 6 chunks from token_based strategy\n",
      "\n",
      "ğŸ“Š Total strategies loaded: 4\n"
     ]
    }
   ],
   "source": [
    "# Load chunks from different strategies\n",
    "strategies = ['hybrid', 'semantic', 'recursive', 'token_based']\n",
    "strategy_chunks = {}\n",
    "\n",
    "for strategy in strategies:\n",
    "    strategy_path = Path(f\"../../data-ingestion/processed/chunked/{strategy}/\")\n",
    "    if strategy_path.exists():\n",
    "        chunks = load_arxiv_chunks(strategy_path, max_files=2, chunks_per_file=3)\n",
    "        strategy_chunks[strategy] = chunks\n",
    "        print(f\"âœ… Loaded {len(chunks)} chunks from {strategy} strategy\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Total strategies loaded: {len(strategy_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "faaaba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adi/Desktop/Projects/agentic-rag/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/adi/Desktop/Projects/agentic-rag/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() iterable argument is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m contents = [c[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[32m      8\u001b[39m avg_length = np.mean([\u001b[38;5;28mlen\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m contents])\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m max_length = \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m min_length = \u001b[38;5;28mmin\u001b[39m([\u001b[38;5;28mlen\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m contents])\n\u001b[32m     12\u001b[39m strategy_stats.append({\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mStrategy\u001b[39m\u001b[33m'\u001b[39m: strategy,\n\u001b[32m     14\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mNum Chunks\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(chunks),\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMax Length\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(max_length)\n\u001b[32m     18\u001b[39m })\n",
      "\u001b[31mValueError\u001b[39m: max() iterable argument is empty"
     ]
    }
   ],
   "source": [
    "# Analyze characteristics of different chunking strategies\n",
    "import pandas as pd\n",
    "\n",
    "strategy_stats = []\n",
    "\n",
    "for strategy, chunks in strategy_chunks.items():\n",
    "    contents = [c['content'] for c in chunks]\n",
    "    avg_length = np.mean([len(c) for c in contents])\n",
    "    max_length = max([len(c) for c in contents])\n",
    "    min_length = min([len(c) for c in contents])\n",
    "    \n",
    "    strategy_stats.append({\n",
    "        'Strategy': strategy,\n",
    "        'Num Chunks': len(chunks),\n",
    "        'Avg Length': int(avg_length),\n",
    "        'Min Length': int(min_length),\n",
    "        'Max Length': int(max_length)\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(strategy_stats)\n",
    "print(\"ğŸ“Š Chunking Strategy Comparison:\\n\")\n",
    "print(stats_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nğŸ’¡ Different strategies create chunks of varying sizes\")\n",
    "print(\"ğŸ’¡ This affects the granularity and context of embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdba3a1",
   "metadata": {},
   "source": [
    "## Comparing Different Chunking Strategies\n",
    "\n",
    "Let's compare embeddings from different chunking strategies (hybrid, semantic, recursive, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b508e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced semantic search with research papers\n",
    "def semantic_search_arxiv(query, documents, doc_metadata, embeddings_model, top_k=5):\n",
    "    \"\"\"Search through ArXiv paper chunks with detailed results\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ” Searching for: '{query}'\")\n",
    "    print(f\"ğŸ“Š Searching through {len(documents)} chunks...\")\n",
    "    \n",
    "    # Create embeddings\n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "    doc_embeddings = embeddings_model.embed_documents(documents)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    def cosine_similarity(vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_a = np.linalg.norm(vec1)\n",
    "        norm_b = np.linalg.norm(vec2)\n",
    "        return dot_product / (norm_a * norm_b)\n",
    "    \n",
    "    similarities = []\n",
    "    for i, doc_emb in enumerate(doc_embeddings):\n",
    "        similarity = cosine_similarity(query_embedding, doc_emb)\n",
    "        similarities.append((similarity, documents[i], doc_metadata[i]))\n",
    "    \n",
    "    # Sort and return top results\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Test different queries\n",
    "test_queries = [\n",
    "    \"neural networks and deep learning\",\n",
    "    \"machine learning algorithms\",\n",
    "    \"data processing and analysis\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    results = semantic_search_arxiv(\n",
    "        query, \n",
    "        paper_contents, \n",
    "        [doc['metadata'] for doc in arxiv_chunks],\n",
    "        embeddings,\n",
    "        top_k=3\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ† Top 3 Results:\\n\")\n",
    "    for i, (score, content, metadata) in enumerate(results, 1):\n",
    "        print(f\"{i}. Similarity: {score:.4f}\")\n",
    "        print(f\"   ğŸ“„ Paper: {metadata['title'][:70]}...\")\n",
    "        print(f\"   ğŸ“‚ Section: {metadata['section']}\")\n",
    "        print(f\"   ğŸ†” ArXiv ID: {metadata['arxiv_id']}\")\n",
    "        print(f\"   ğŸ“ Preview: {content[:150]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for all loaded research papers\n",
    "paper_contents = [doc['content'] for doc in arxiv_chunks]\n",
    "paper_embeddings = embeddings.embed_documents(paper_contents)\n",
    "\n",
    "print(f\"âœ… Created embeddings for {len(paper_embeddings)} research paper chunks\")\n",
    "print(f\"ğŸ“ Each embedding: {len(paper_embeddings[0])} dimensions\")\n",
    "print(f\"ğŸ’¾ Total size: ~{len(paper_embeddings) * len(paper_embeddings[0]) * 4 / 1024:.2f} KB\")\n",
    "print(f\"\\nğŸ“š Papers include:\")\n",
    "unique_titles = set([doc['metadata']['title'] for doc in arxiv_chunks])\n",
    "for i, title in enumerate(list(unique_titles)[:5], 1):\n",
    "    print(f\"   {i}. {title[:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e12d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.float64(0.6227387139613365),\n",
       "  'Embeddings convert text into numerical vectors'),\n",
       " (np.float64(0.25206899523723963),\n",
       "  'Machine learning is a subset of artificial intelligence'),\n",
       " (np.float64(0.2291701911027054),\n",
       "  'LangChain is a framework for developing applications powered by language models')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another query on research papers\n",
    "query = \"What is deep learning and neural network architecture?\"\n",
    "results = semantic_search(query, documents, embeddings)\n",
    "\n",
    "print(f\"\\nğŸ” Semantic Search Results for: '{query}'\\n\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for i, (score, doc) in enumerate(results, 1):\n",
    "    idx = documents.index(doc)\n",
    "    metadata = arxiv_chunks[idx]['metadata']\n",
    "    \n",
    "    print(f\"\\n{i}. Similarity: {score:.4f}\")\n",
    "    print(f\"   ğŸ“„ Paper: {metadata['title'][:65]}...\")\n",
    "    print(f\"   ğŸ“‚ Section: {metadata['section']}\")\n",
    "    print(f\"   ğŸ“ Content: {doc[:120]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c672c42",
   "metadata": {},
   "source": [
    "## HuggingFace vs OpenAI Embeddings Comparison\n",
    "\n",
    "| **Aspect**               | **HuggingFace (Sentence Transformers)**                      | **OpenAI Embeddings**                                |\n",
    "| ------------------------ | ------------------------------------------------------------ | ---------------------------------------------------- |\n",
    "| **Cost**                 | âœ… **FREE** - Runs locally                                   | âŒ Pay per token (~$0.02-0.13 per 1M tokens)        |\n",
    "| **API Key Required**     | âœ… **No** - Works offline                                    | âŒ Yes - Need OpenAI API key                        |\n",
    "| **Privacy**              | âœ… Data stays local                                          | âŒ Data sent to OpenAI servers                      |\n",
    "| **Speed**                | âš¡ Fast (local GPU/CPU)                                      | ğŸŒ Depends on network + API latency                 |\n",
    "| **Dimensions**           | 384-768 typical                                              | 1536-3072                                            |\n",
    "| **Quality**              | ğŸŸ¢ Excellent for most tasks                                  | ğŸŸ¢ Slightly better on some benchmarks               |\n",
    "| **Customization**        | âœ… Fine-tune on your data                                    | âŒ No customization                                  |\n",
    "| **Best For**             | RAG systems, semantic search, production apps with budget    | Enterprise apps with budget, maximum accuracy       |\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "**Semantic Search Performance:**\n",
    "- Both understand meaning and context\n",
    "- HuggingFace models are optimized for semantic similarity\n",
    "- OpenAI models may perform slightly better on very complex queries\n",
    "\n",
    "**Practical Considerations:**\n",
    "- **Development**: HuggingFace is perfect (free, fast, no API limits)\n",
    "- **Production**: HuggingFace scales better (no per-request costs)\n",
    "- **Enterprise**: OpenAI if you need absolute best quality and have budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1625a37d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
