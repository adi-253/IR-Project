{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8fa17d0",
   "metadata": {},
   "source": [
    "# Chunking Strategies Demo\n",
    "## Demonstrating 4 Different Chunking Approaches for ArXiv Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da29648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adi/Desktop/Projects/agentic-rag/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21d88e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adi/Desktop/Projects/agentic-rag/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import re\n",
    "\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f4737",
   "metadata": {},
   "source": [
    "## Load Sample Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1643850a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Working directory: /home/adi/Desktop/Projects/agentic-rag/data-ingestion\n",
      "ðŸ“‚ Project root: /home/adi/Desktop/Projects/agentic-rag/data-ingestion\n",
      "ðŸ“‚ Text directory: /home/adi/Desktop/Projects/agentic-rag/data-ingestion/raw_data/arxiv/text\n",
      "ðŸ“‚ Text dir exists: True\n",
      "ðŸ“‚ Files found: 100\n",
      "\n",
      "ðŸ“„ Paper: Amortized Inference of Neuron Parameters on Analog Neuromorphic Hardware...\n",
      "ðŸ“Œ ArXiv ID: 2602.10763v2\n",
      "ðŸ“ Category: cs.NE\n",
      "ðŸ“ Text Length: 37,293 characters\n",
      "ðŸ“– Pages: 8\n"
     ]
    }
   ],
   "source": [
    "# Load first paper for demonstration\n",
    "import os\n",
    "\n",
    "# Get the notebook's directory and project root\n",
    "notebook_dir = Path.cwd()\n",
    "\n",
    "# Try multiple strategies to find the project root\n",
    "if (notebook_dir / \"raw_data\").exists():\n",
    "    project_root = notebook_dir\n",
    "elif (notebook_dir.parent / \"raw_data\").exists():\n",
    "    project_root = notebook_dir.parent\n",
    "elif (notebook_dir / \"data-ingestion\" / \"raw_data\").exists():\n",
    "    project_root = notebook_dir / \"data-ingestion\"\n",
    "else:\n",
    "    # Fallback: assume we're in data-ingestion/src\n",
    "    project_root = Path(\"/home/adi/Desktop/Projects/agentic-rag/data-ingestion\")\n",
    "\n",
    "text_dir = project_root / \"raw_data\" / \"arxiv\" / \"text\"\n",
    "metadata_dir = project_root / \"raw_data\" / \"arxiv\" / \"metadata\"\n",
    "\n",
    "print(f\"ðŸ“‚ Working directory: {Path.cwd()}\")\n",
    "print(f\"ðŸ“‚ Project root: {project_root}\")\n",
    "print(f\"ðŸ“‚ Text directory: {text_dir}\")\n",
    "print(f\"ðŸ“‚ Text dir exists: {text_dir.exists()}\")\n",
    "print(f\"ðŸ“‚ Files found: {len(list(text_dir.glob('*.txt'))) if text_dir.exists() else 0}\\n\")\n",
    "\n",
    "# Get first text file\n",
    "text_files = sorted(text_dir.glob(\"*.txt\"))\n",
    "if not text_files:\n",
    "    raise FileNotFoundError(f\"No text files found in {text_dir}\")\n",
    "sample_file = text_files[0]\n",
    "\n",
    "# Load text\n",
    "with open(sample_file, 'r', encoding='utf-8') as f:\n",
    "    paper_text = f.read()\n",
    "\n",
    "# Load metadata\n",
    "metadata_file = metadata_dir / f\"{sample_file.stem}.json\"\n",
    "with open(metadata_file, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"ðŸ“„ Paper: {metadata['title'][:80]}...\")\n",
    "print(f\"ðŸ“Œ ArXiv ID: {metadata['arxiv_id']}\")\n",
    "print(f\"ðŸ“ Category: {metadata['primary_category']}\")\n",
    "print(f\"ðŸ“ Text Length: {len(paper_text):,} characters\")\n",
    "print(f\"ðŸ“– Pages: {metadata.get('num_pages', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a8579ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“– Paper Preview:\n",
      "================================================================================\n",
      "Amortized Inference of Neuron Parameters on\n",
      "Analog Neuromorphic Hardware\n",
      "Jakob Kaiserâˆ—â€ , Eric MÃ¼llerâ€¡ and Johannes Schemmelâ€ \n",
      "â€ Institute of Computer Engineering, Heidelberg University, Germany\n",
      "â€¡Kirchhoff Institute for Physics, Heidelberg University, Germany\n",
      "âˆ—jakob.kaiser@kip.uni-heidelberg.de\n",
      "This article has been accepted for presentation at the Neuro Inspired Computational Elements Conference 2026 and will appear in the conference proceedings.\n",
      "Â©2026 IEEE. Personal use of this material is permit\n",
      "...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Preview first 500 characters\n",
    "print(\"\\nðŸ“– Paper Preview:\")\n",
    "print(\"=\" * 80)\n",
    "print(paper_text[:500])\n",
    "print(\"...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a441e88",
   "metadata": {},
   "source": [
    "## Strategy 1: Recursive Character Splitting (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e33f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Recursive Character Splitting\n",
      "   Chunk Size: 1000 chars\n",
      "   Overlap: 200 chars\n",
      "   Total Chunks: 49\n",
      "\n",
      "ðŸ“Š Chunk Sizes:\n",
      "   Min: 343 chars\n",
      "   Max: 999 chars\n",
      "   Avg: 906 chars\n"
     ]
    }
   ],
   "source": [
    "# Recursive character splitter\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "recursive_chunks = recursive_splitter.split_text(paper_text)\n",
    "\n",
    "print(f\"ðŸ”¹ Recursive Character Splitting\")\n",
    "print(f\"   Chunk Size: 1000 chars\")\n",
    "print(f\"   Overlap: 200 chars\")\n",
    "print(f\"   Total Chunks: {len(recursive_chunks)}\")\n",
    "print(f\"\\nðŸ“Š Chunk Sizes:\")\n",
    "print(f\"   Min: {min(len(c) for c in recursive_chunks)} chars\")\n",
    "print(f\"   Max: {max(len(c) for c in recursive_chunks)} chars\")\n",
    "print(f\"   Avg: {sum(len(c) for c in recursive_chunks) // len(recursive_chunks)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8238494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‘ First 3 Chunks:\n",
      "================================================================================\n",
      "\n",
      "--- Chunk 1 (951 chars) ---\n",
      "Amortized Inference of Neuron Parameters on\n",
      "Analog Neuromorphic Hardware\n",
      "Jakob Kaiserâˆ—â€ , Eric MÃ¼llerâ€¡ and Johannes Schemmelâ€ \n",
      "â€ Institute of Computer Engineering, Heidelberg University, Germany\n",
      "â€¡Kirchhoff Institute for Physics, Heidelberg University, Germany\n",
      "âˆ—jakob.kaiser@kip.uni-heidelberg.de\n",
      "This ar...\n",
      "\n",
      "\n",
      "--- Chunk 2 (952 chars) ---\n",
      "reuse of any copyrighted component of this work in other works.\n",
      "Abstractâ€”Our work utilized a non-sequential simulation-based\n",
      "inference algorithm to provide an amortized neural density\n",
      "estimator, which approximates the posterior distribution for seven\n",
      "parameters of the adaptive exponential integrate-...\n",
      "\n",
      "\n",
      "--- Chunk 3 (975 chars) ---\n",
      "a more focused posterior and generated posterior predictive traces\n",
      "that accurately captured the membrane potential dynamics. When\n",
      "using handcrafted summary statistics, posterior predictive traces\n",
      "match the included features but show deviations in the exact\n",
      "dynamics. The posteriors showed signs of bi...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show first 3 chunks\n",
    "print(\"\\nðŸ“‘ First 3 Chunks:\")\n",
    "print(\"=\" * 80)\n",
    "for i, chunk in enumerate(recursive_chunks[:3], 1):\n",
    "    print(f\"\\n--- Chunk {i} ({len(chunk)} chars) ---\")\n",
    "    print(chunk[:300] + \"...\" if len(chunk) > 300 else chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa7264",
   "metadata": {},
   "source": [
    "## Strategy 2: Token-Based Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6152c3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Token-Based Chunking\n",
      "   Max Tokens: 512\n",
      "   Overlap: 50 tokens\n",
      "   Total Chunks: 21\n",
      "\n",
      "ðŸ“Š Chunk Sizes (in characters):\n",
      "   Min: 209 chars\n",
      "   Max: 2528 chars\n",
      "   Avg: 1970 chars\n"
     ]
    }
   ],
   "source": [
    "# Token-based splitter\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(paper_text)\n",
    "\n",
    "print(f\"ðŸ”¹ Token-Based Chunking\")\n",
    "print(f\"   Max Tokens: 512\")\n",
    "print(f\"   Overlap: 50 tokens\")\n",
    "print(f\"   Total Chunks: {len(token_chunks)}\")\n",
    "print(f\"\\nðŸ“Š Chunk Sizes (in characters):\")\n",
    "print(f\"   Min: {min(len(c) for c in token_chunks)} chars\")\n",
    "print(f\"   Max: {max(len(c) for c in token_chunks)} chars\")\n",
    "print(f\"   Avg: {sum(len(c) for c in token_chunks) // len(token_chunks)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "430457d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‘ First 2 Token Chunks:\n",
      "================================================================================\n",
      "\n",
      "--- Token Chunk 1 (2404 chars) ---\n",
      "Amortized Inference of Neuron Parameters on\n",
      "Analog Neuromorphic Hardware\n",
      "Jakob Kaiserâˆ—â€ , Eric MÃ¼llerâ€¡ and Johannes Schemmelâ€ \n",
      "â€ Institute of Computer Engineering, Heidelberg University, Germany\n",
      "â€¡Kirchhoff Institute for Physics, Heidelberg University, Germany\n",
      "âˆ—jakob.kaiser@kip.uni-heidelberg.de\n",
      "This ar...\n",
      "\n",
      "\n",
      "--- Token Chunk 2 (2308 chars) ---\n",
      " BayesFlow\n",
      "I. INTRODUCTION\n",
      "Identifying suitable models to replicate physical observations\n",
      "is often a laborious process. Even when a model is available,\n",
      "determining an effective parameterization can be challenging.\n",
      "Various methods have been used in neuroscience to achieve\n",
      "appropriate model parameteri...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show first 2 token chunks\n",
    "print(\"\\nðŸ“‘ First 2 Token Chunks:\")\n",
    "print(\"=\" * 80)\n",
    "for i, chunk in enumerate(token_chunks[:2], 1):\n",
    "    print(f\"\\n--- Token Chunk {i} ({len(chunk)} chars) ---\")\n",
    "    print(chunk[:300] + \"...\" if len(chunk) > 300 else chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e2573",
   "metadata": {},
   "source": [
    "## Strategy 3: Section-Based Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecd8d530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Section-Based Chunking\n",
      "   Sections Found: 28\n",
      "\n",
      "ðŸ“‘ Section Breakdown:\n",
      "   â€¢ Abstract: 1,153 chars\n",
      "   â€¢ results: 181 chars\n",
      "   â€¢ INTRODUCTION: 194 chars\n",
      "   â€¢ methods: 84 chars\n",
      "   â€¢ methods: 115 chars\n",
      "   â€¢ methods: 83 chars\n",
      "   â€¢ methods: 185 chars\n",
      "   â€¢ methods: 3,680 chars\n",
      "   â€¢ results: 1,129 chars\n",
      "   â€¢ METHODS: 3,910 chars\n",
      "   â€¢ experiments: 72 chars\n",
      "   â€¢ approach: 1,027 chars\n",
      "   â€¢ RESULTS: 329 chars\n",
      "   â€¢ results: 713 chars\n",
      "   â€¢ experiments: 4,706 chars\n",
      "   â€¢ experiments: 2,296 chars\n",
      "   â€¢ experiments: 2,923 chars\n",
      "   â€¢ DISCUSSION: 5,562 chars\n",
      "   â€¢ results: 650 chars\n",
      "   â€¢ Conclusion: 409 chars\n",
      "   â€¢ results: 228 chars\n",
      "   â€¢ methods: 584 chars\n",
      "   â€¢ approach: 1,060 chars\n",
      "   â€¢ methodology: 81 chars\n",
      "   â€¢ methodology: 58 chars\n",
      "   â€¢ methodology: 97 chars\n",
      "   â€¢ REFERENCES: 325 chars\n",
      "   â€¢ Methods: 4,337 chars\n"
     ]
    }
   ],
   "source": [
    "# Section detection\n",
    "section_patterns = [\n",
    "    r'\\bAbstract\\b',\n",
    "    r'\\b(?:1\\.?\\s+)?Introduction\\b',\n",
    "    r'\\b(?:\\d+\\.?\\s+)?Related\\s+Work\\b',\n",
    "    r'\\b(?:\\d+\\.?\\s+)?(?:Methodology|Methods|Approach)\\b',\n",
    "    r'\\b(?:\\d+\\.?\\s+)?(?:Experiments|Experimental\\s+Results)\\b',\n",
    "    r'\\b(?:\\d+\\.?\\s+)?Results\\b',\n",
    "    r'\\b(?:\\d+\\.?\\s+)?Discussion\\b',\n",
    "    r'\\b(?:\\d+\\.?\\s+)?Conclusion\\b',\n",
    "    r'\\b(?:References|Bibliography)\\b'\n",
    "]\n",
    "\n",
    "def extract_sections(text):\n",
    "    sections = []\n",
    "    section_positions = []\n",
    "    \n",
    "    for pattern in section_patterns:\n",
    "        for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "            section_name = match.group(0).strip()\n",
    "            section_positions.append((match.start(), section_name))\n",
    "    \n",
    "    section_positions.sort()\n",
    "    \n",
    "    for i, (start_pos, section_name) in enumerate(section_positions):\n",
    "        if i + 1 < len(section_positions):\n",
    "            end_pos = section_positions[i + 1][0]\n",
    "        else:\n",
    "            end_pos = len(text)\n",
    "        \n",
    "        section_text = text[start_pos:end_pos].strip()\n",
    "        section_text = re.sub(section_name, '', section_text, count=1).strip()\n",
    "        \n",
    "        if section_text:\n",
    "            sections.append((section_name, section_text))\n",
    "    \n",
    "    if not sections:\n",
    "        sections.append((\"Full Text\", text))\n",
    "    \n",
    "    return sections\n",
    "\n",
    "sections = extract_sections(paper_text)\n",
    "\n",
    "print(f\"ðŸ”¹ Section-Based Chunking\")\n",
    "print(f\"   Sections Found: {len(sections)}\")\n",
    "print(f\"\\nðŸ“‘ Section Breakdown:\")\n",
    "for section_name, section_text in sections:\n",
    "    print(f\"   â€¢ {section_name}: {len(section_text):,} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04c74fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Total Section-Based Chunks: 58\n",
      "\n",
      "ðŸ“‘ Sample Section Chunks:\n",
      "================================================================================\n",
      "\n",
      "--- Chunk 1: Abstract (1153 chars) ---\n",
      "â€”Our work utilized a non-sequential simulation-based\n",
      "inference algorithm to provide an amortized neural density\n",
      "estimator, which approximates the posterior distribution for seven\n",
      "parameters of the adaptive exponential integrate-and-fire neuron\n",
      "model ...\n",
      "\n",
      "\n",
      "--- Chunk 2: results (181 chars) ---\n",
      "validate amortized simulation-based\n",
      "inference as a tool for parameterizing analog neuron circuits.\n",
      "Index Termsâ€”Neuromorphic Computing, Simulation-Based\n",
      "Inference, AdEx, BayesFlow\n",
      "I.\n",
      "\n",
      "\n",
      "--- Chunk 3: INTRODUCTION (194 chars) ---\n",
      "Identifying suitable models to replicate physical observations\n",
      "is often a laborious process. Even when a model is available,\n",
      "determining an effective parameterization can be challenging.\n",
      "Various\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create chunks from sections (split large sections)\n",
    "section_chunks = []\n",
    "for section_name, section_text in sections:\n",
    "    if len(section_text) > 2000:\n",
    "        # Split large sections\n",
    "        subsplitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        subchunks = subsplitter.split_text(section_text)\n",
    "        for subchunk in subchunks:\n",
    "            section_chunks.append((section_name, subchunk))\n",
    "    else:\n",
    "        section_chunks.append((section_name, section_text))\n",
    "\n",
    "print(f\"\\nðŸ“Š Total Section-Based Chunks: {len(section_chunks)}\")\n",
    "print(\"\\nðŸ“‘ Sample Section Chunks:\")\n",
    "print(\"=\" * 80)\n",
    "for i, (section, chunk) in enumerate(section_chunks[:3], 1):\n",
    "    print(f\"\\n--- Chunk {i}: {section} ({len(chunk)} chars) ---\")\n",
    "    print(chunk[:250] + \"...\" if len(chunk) > 250 else chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b989e561",
   "metadata": {},
   "source": [
    "## Strategy 4: Hybrid Chunking (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35bd5f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Hybrid Chunking\n",
      "   Total Chunks: 61\n",
      "\n",
      "ðŸ“Š Strategy Distribution:\n",
      "   â€¢ Whole Sections: 15\n",
      "   â€¢ Smart Split: 46\n"
     ]
    }
   ],
   "source": [
    "# Hybrid approach: section-aware + smart splitting\n",
    "hybrid_chunks = []\n",
    "\n",
    "for section_name, section_text in sections:\n",
    "    if len(section_text) < 100:\n",
    "        continue\n",
    "    \n",
    "    # Keep short sections whole\n",
    "    if section_name.lower() in ['abstract', 'conclusion'] or len(section_text) < 1500:\n",
    "        hybrid_chunks.append({\n",
    "            'section': section_name,\n",
    "            'content': section_text,\n",
    "            'strategy': 'whole_section'\n",
    "        })\n",
    "    else:\n",
    "        # Smart split for larger sections\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=800,\n",
    "            chunk_overlap=150,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        chunks = splitter.split_text(section_text)\n",
    "        for chunk in chunks:\n",
    "            hybrid_chunks.append({\n",
    "                'section': section_name,\n",
    "                'content': chunk,\n",
    "                'strategy': 'smart_split'\n",
    "            })\n",
    "\n",
    "print(f\"ðŸ”¹ Hybrid Chunking\")\n",
    "print(f\"   Total Chunks: {len(hybrid_chunks)}\")\n",
    "print(f\"\\nðŸ“Š Strategy Distribution:\")\n",
    "whole_count = sum(1 for c in hybrid_chunks if c['strategy'] == 'whole_section')\n",
    "split_count = sum(1 for c in hybrid_chunks if c['strategy'] == 'smart_split')\n",
    "print(f\"   â€¢ Whole Sections: {whole_count}\")\n",
    "print(f\"   â€¢ Smart Split: {split_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bc88bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‘ Sample Hybrid Chunks:\n",
      "================================================================================\n",
      "\n",
      "--- Hybrid Chunk 1 ---\n",
      "Section: Abstract\n",
      "Strategy: whole_section\n",
      "Length: 1153 chars\n",
      "\n",
      "Content Preview:\n",
      "â€”Our work utilized a non-sequential simulation-based\n",
      "inference algorithm to provide an amortized neural density\n",
      "estimator, which approximates the posterior distribution for seven\n",
      "parameters of the adaptive exponential integrate-and-fire neuron\n",
      "model of the analog neuromorphic BrainScaleS-2 substrate...\n",
      "\n",
      "\n",
      "--- Hybrid Chunk 2 ---\n",
      "Section: results\n",
      "Strategy: whole_section\n",
      "Length: 181 chars\n",
      "\n",
      "Content Preview:\n",
      "validate amortized simulation-based\n",
      "inference as a tool for parameterizing analog neuron circuits.\n",
      "Index Termsâ€”Neuromorphic Computing, Simulation-Based\n",
      "Inference, AdEx, BayesFlow\n",
      "I.\n",
      "\n",
      "\n",
      "--- Hybrid Chunk 3 ---\n",
      "Section: INTRODUCTION\n",
      "Strategy: whole_section\n",
      "Length: 194 chars\n",
      "\n",
      "Content Preview:\n",
      "Identifying suitable models to replicate physical observations\n",
      "is often a laborious process. Even when a model is available,\n",
      "determining an effective parameterization can be challenging.\n",
      "Various\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show hybrid chunks with metadata\n",
    "print(\"\\nðŸ“‘ Sample Hybrid Chunks:\")\n",
    "print(\"=\" * 80)\n",
    "for i, chunk_data in enumerate(hybrid_chunks[:3], 1):\n",
    "    print(f\"\\n--- Hybrid Chunk {i} ---\")\n",
    "    print(f\"Section: {chunk_data['section']}\")\n",
    "    print(f\"Strategy: {chunk_data['strategy']}\")\n",
    "    print(f\"Length: {len(chunk_data['content'])} chars\")\n",
    "    print(f\"\\nContent Preview:\")\n",
    "    content = chunk_data['content']\n",
    "    print(content[:300] + \"...\" if len(content) > 300 else content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cfa514",
   "metadata": {},
   "source": [
    "## Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7418c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š CHUNKING STRATEGIES COMPARISON\n",
      "====================================================================================================\n",
      "            Strategy  Total Chunks  Avg Size (chars)                      Best For\n",
      " Recursive Character            49               906   General retrieval, baseline\n",
      "         Token-Based            21              1970 Direct LLM input (512 tokens)\n",
      "       Section-Based            58               694 Structured queries by section\n",
      "Hybrid (Recommended)            61               643   Best overall, section-aware\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison table\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Strategy': 'Recursive Character',\n",
    "        'Total Chunks': len(recursive_chunks),\n",
    "        'Avg Size (chars)': sum(len(c) for c in recursive_chunks) // len(recursive_chunks),\n",
    "    },\n",
    "    {\n",
    "        'Strategy': 'Token-Based',\n",
    "        'Total Chunks': len(token_chunks),\n",
    "        'Avg Size (chars)': sum(len(c) for c in token_chunks) // len(token_chunks),\n",
    "    },\n",
    "    {\n",
    "        'Strategy': 'Section-Based',\n",
    "        'Total Chunks': len(section_chunks),\n",
    "        'Avg Size (chars)': sum(len(c[1]) for c in section_chunks) // len(section_chunks),\n",
    "    },\n",
    "    {\n",
    "        'Strategy': 'Hybrid (Recommended)',\n",
    "        'Total Chunks': len(hybrid_chunks),\n",
    "        'Avg Size (chars)': sum(len(c['content']) for c in hybrid_chunks) // len(hybrid_chunks),\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\nðŸ“Š CHUNKING STRATEGIES COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103386ef",
   "metadata": {},
   "source": [
    "## Load Pre-computed Chunks from Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71bfaf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Available Chunked Data from Pipeline:\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load pre-computed chunks from pipeline\n",
    "chunked_dir = Path(\"../processed/chunked\")\n",
    "\n",
    "print(\"ðŸ“ Available Chunked Data from Pipeline:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for strategy in ['recursive', 'token_based', 'section_based', 'hybrid']:\n",
    "    strategy_dir = chunked_dir / strategy\n",
    "    if strategy_dir.exists():\n",
    "        chunk_files = list(strategy_dir.glob(\"*.json\"))\n",
    "        print(f\"\\nðŸ”¹ {strategy.replace('_', ' ').title()}\")\n",
    "        print(f\"   Papers Chunked: {len(chunk_files)}\")\n",
    "        \n",
    "        # Load one example\n",
    "        if chunk_files:\n",
    "            with open(chunk_files[0], 'r') as f:\n",
    "                chunks = json.load(f)\n",
    "            print(f\"   Chunks per paper (sample): {len(chunks)}\")\n",
    "            print(f\"   First chunk preview: {chunks[0]['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc197d4",
   "metadata": {},
   "source": [
    "## Statistics Across All Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d25cc969",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../logs/chunking_summary.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load summary statistics\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../logs/chunking_summary.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m     summary = json.load(f)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ“Š COMPLETE COLLECTION STATISTICS\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/agentic-rag/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../logs/chunking_summary.json'"
     ]
    }
   ],
   "source": [
    "# Load summary statistics\n",
    "with open('../logs/chunking_summary.json', 'r') as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "print(\"\\nðŸ“Š COMPLETE COLLECTION STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal Papers Processed: {summary['papers_processed']}\")\n",
    "print(f\"\\nChunks Created:\")\n",
    "print(f\"  â€¢ Recursive Character: {summary['recursive_chunks']:,}\")\n",
    "print(f\"  â€¢ Token-Based: {summary['token_chunks']:,}\")\n",
    "print(f\"  â€¢ Section-Based: {summary['section_chunks']:,}\")\n",
    "print(f\"  â€¢ Hybrid: {summary['hybrid_chunks']:,}\")\n",
    "print(f\"\\nTotal Chunks: {sum([summary['recursive_chunks'], summary['token_chunks'], summary['section_chunks'], summary['hybrid_chunks']]):,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb3987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
